{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contain the experiment and the code used for our paper for the binary path environment.\n",
    "\n",
    "It is advised to open it with the collapse heading option as it is quite heavy.\n",
    "\n",
    "The notebook is composed of 5 parts :\n",
    "    -We define the environment\n",
    "    -We define the Thompson sampling algorithm for this environment\n",
    "    -We define the ESCB algorithm for this environment\n",
    "    -We experiment on the first time the optimal decision is played, with our without forced exploration\n",
    "    -We compare the regret between ESCB and Thompson sampling, with or without forced exploration\n",
    "\n",
    "It is not advised to launch the notebook as it is :\n",
    "    -You should change the saving path of the figures\n",
    "    -Reduce the number of experiment, the time horizon, \n",
    "    -Increase the gap (delta) \n",
    "    -Change the range of the number of arms if needed\n",
    "Our parameters have been chosen to reduce the variance by computing more sample path, and using large time horizon. (As we are showing that a method do not work well you could wait a long time before getting any results with those parameters as we did it once for our paper figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial import\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# import the package needed to solve the environnement\n",
    "from scipy.optimize import linear_sum_assignment #hungarian matching algorithm\n",
    "import numpy as np\n",
    "from scipy.stats import beta, bernoulli,norm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.optimize import root,root_scalar\n",
    "\n",
    "from statsmodels.distributions.empirical_distribution import ECDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2 Decisions environment\n",
    "\n",
    "We create the 2 paths environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Two_decision_env():\n",
    "    \"\"\"\n",
    "    Create an environment with 2 decisions composed of m items with reward a and b with a>b>1/2\n",
    "    \n",
    "    inputs :\n",
    "        - m number of items of a decision\n",
    "        - a parameter of the optimal arms decision\n",
    "        - b parameter of the non-optimal arms decision\n",
    "        - sigma variance for the gaussian distribution \n",
    "        - random_variable = \"bernoulli\", \"gaussian\" distribution of the arm\n",
    "       \n",
    "    \"\"\" \n",
    "    \n",
    "    \n",
    "    def __init__(self,m,a = 1,b = 0.9,sigma = 1,random_variable = \"bernoulli\"):\n",
    "        self.m = m\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        self.random_variable = random_variable\n",
    "                 \n",
    "    def draw(self, decision):\n",
    "        \"\"\"\n",
    "        Draw a vector of reward\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.random_variable == \"bernoulli\":\n",
    "            if decision == 0:\n",
    "                reward = bernoulli.rvs(self.a, size = self.m)\n",
    "            elif decision == 1:\n",
    "                reward = bernoulli.rvs(self.b, size = self.m)\n",
    "                \n",
    "        elif self.random_variable == \"gaussian\":\n",
    "            if decision == 0:\n",
    "                reward = norm.rvs(self.a, scale=self.sigma, size = self.m)\n",
    "            elif decision == 1:\n",
    "                reward = norm.rvs(self.b, scale=self.sigma, size = self.m)\n",
    "            \n",
    "        if decision == 1:\n",
    "            regret = (self.a-self.b)*self.m\n",
    "        elif decision == 0:\n",
    "            regret = 0\n",
    "            \n",
    "        return reward, regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ESCB\n",
    "\n",
    "We define the ESCB for the 2 paths environment. ESCB like CUCB cumpute track the mean of each arm and number of time they were played. With that information it computes optimistic indexes for all the decision and find the decision that maximise those indexes. See (Combes et al 2015) for an explanation in details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def newton(f,df,x0):\n",
    "#     xn = x0\n",
    "#     for n in range(100):\n",
    "#         fxn = f(xn)\n",
    "#         dfxn = df(xn)\n",
    "#         if np.abs(fxn) < 10**-3:\n",
    "#             return(xn)\n",
    "#         xn = xn - fxn/dfxn\n",
    "#         print(dfxn)\n",
    "#     print(\"Zero not found\")\n",
    "#     return(xn)\n",
    "\n",
    "# def dicho(f,a,b):\n",
    "    \n",
    "#     for n in range(100):\n",
    "#         fa = f(a)\n",
    "#         fb = f(b)\n",
    "#         fxn = f(xn)\n",
    "#         if np.abs(fxn) < 10**-3:\n",
    "#             return(xn)\n",
    "#         fab = 3\n",
    "#         if fxn < 0:\n",
    "#             a =5+6\n",
    "#         if fxn > 0:\n",
    "#             a = 5+7\n",
    "#         print(dfxn)\n",
    "#     print(\"Zero not found\")\n",
    "#     return(xn)\n",
    "\n",
    "class ESCB():\n",
    "    \"\"\"\n",
    "    TS for the two decision environnement\n",
    "    \n",
    "    inputs : \n",
    "        -m number of arm of one decision\n",
    "        -n_init number of initialisation step (2\\ell in our paper)\n",
    "        -index of the ESCB algorithm (we used the 2 second) the first one is not implemented\n",
    "    \"\"\"\n",
    "    def __init__(self, m,n_init = 0,index = 2):\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.m = m\n",
    "    \n",
    "        self.numplayed = np.zeros(2)\n",
    "        \n",
    "        self.mu = np.zeros(2*self.m)\n",
    "        self.sigma = np.zeros(2*self.m)\n",
    "        \n",
    "        self.initialization = False\n",
    "        \n",
    "        self.n_init = n_init # sample n_init time each decision\n",
    "        if self.n_init > 0:   \n",
    "            self.initialization = True\n",
    "        \n",
    "        self.playhist = []\n",
    "        \n",
    "        self.index = index\n",
    "        \n",
    "         \n",
    "    def reset(self):\n",
    "        self.iteration = 0\n",
    "        self.weight_draw = np.zeros(2*m)\n",
    "        self.numplayed = np.zeros(2)\n",
    "        self.playhist = []\n",
    "        \n",
    "        if self.n_init > 0:   \n",
    "            self.initialization = True\n",
    "        \n",
    "        \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        This will compute indexes and find the decision that maximise.\n",
    "        \n",
    "        return a decision\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.iteration < self.n_init:\n",
    "            self.decision = 0\n",
    "            self.playhist.append(self.decision)\n",
    "            return(self.decision)\n",
    "        \n",
    "        if self.iteration < 2*self.n_init:\n",
    "            self.decision = 1\n",
    "            self.playhist.append(self.decision)\n",
    "            return(self.decision)\n",
    "        else:       \n",
    "            self.initialization = False\n",
    "        \n",
    "        # can be done better, the derivativ can be provided\n",
    "        if self.index == 1:\n",
    "            function1, dfunction1 = self.fgenerator(self.mu[:self.m],0)\n",
    "            index_a = np.sum(newton(function1,dfunction1,np.ones(self.m)*0.5))\n",
    "            #index_a = np.sum(root_scalar(function1, x0 = np.ones(self.m)*0.5,x1 = np.ones(self.m)).root)\n",
    "\n",
    "            function1, dfunction1 = self.fgenerator(self.mu[self.m:],0)\n",
    "            index_b = np.sum(newton(function1,dfunction1,np.ones(self.m)*0.5))\n",
    "            #index_b = np.sum(root_scalar(function1, x0 = np.ones(self.m)*0.5,x1 = np.ones(self.m)).root)\n",
    "\n",
    "\n",
    "            \n",
    "        elif self.index == 2:                 \n",
    "            index_a = np.sum(self.mu[:self.m])+np.sqrt(m/2*self.f()/self.numplayed[0])\n",
    "            index_b = np.sum(self.mu[self.m:])+np.sqrt(m/2*self.f()/self.numplayed[1])\n",
    "            \n",
    "        if index_a > index_b:\n",
    "            self.decision = 0\n",
    "        else :\n",
    "            self.decision = 1\n",
    "            \n",
    "            \n",
    "        self.playhist.append(self.decision)\n",
    "\n",
    "        return self.decision\n",
    "    \n",
    "    def fgenerator(self,p,number):\n",
    "        def function1(q):\n",
    "            return self.kl(p,q)*self.numplayed[number] - self.f()\n",
    "        def dfunction1(q):\n",
    "            return (-p/q + (1-p)/(1-q))*self.numplayed[number]\n",
    "        return function1, dfunction1\n",
    "    \n",
    "    def kl(self,p,q):\n",
    "        \"\"\"\n",
    "        compute the kl divergence of 2 bernoulli distribution with parameter p and q\n",
    "        \"\"\"\n",
    "        return np.sum(p*np.log(p/(q+10**-8))+(1-p)*np.log((1-p+10**-8)/(1-q)))\n",
    "    \n",
    "    def f(self):\n",
    "        \"\"\"\n",
    "        Function used to compute the indexes of the decision\n",
    "        \"\"\"\n",
    "        return np.log(self.iteration)+4*self.m*np.log(np.log(self.iteration))\n",
    "\n",
    "    def update(self, observation):\n",
    "        \"\"\"\n",
    "        update the parameter of the arms\n",
    "        observation are a dictionary of reward\n",
    "        \"\"\"\n",
    "        self.iteration += 1\n",
    "        self.numplayed[self.decision] += 1\n",
    "        \n",
    "        if self.decision == 0:    \n",
    "            self.mu[:self.m] = (self.mu[:self.m]*(self.numplayed[self.decision]-1)+observation)/self.numplayed[self.decision]\n",
    "        elif self.decision == 1:\n",
    "            self.mu[self.m:] = (self.mu[self.m:]*(self.numplayed[self.decision]-1)+observation)/self.numplayed[self.decision]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TS\n",
    "\n",
    "Define the thompson sampling algorithm for the 2 paths environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CTS_exp():\n",
    "    \"\"\"\n",
    "    TS for the two paths environnement\n",
    "    \n",
    "    input : \n",
    "        -m number  of arm in one decision\n",
    "        -n_init number of initialisation step (2 \\ell in our paper)\n",
    "        -post_distrib : beta for bernoulli distribution, gaussian for gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, m,n_init = 0, post_distrib = \"beta\"):\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.m = m\n",
    "        \n",
    "        \n",
    "        self.weight_draw = np.zeros(2*m)\n",
    "        self.numplayed = np.zeros(2)\n",
    "        \n",
    "        self.initialization = False\n",
    "        \n",
    "        self.n_init = n_init # sample n_init time each decision\n",
    "        if self.n_init > 0:   \n",
    "            self.initialization = True\n",
    "        \n",
    "        \n",
    "        self.post_distrib = post_distrib\n",
    "        self.playhist = []\n",
    "        self.decision = 1\n",
    "        \n",
    "        \n",
    "        if self.post_distrib == \"beta\":\n",
    "            self.alpha = np.ones(2*self.m)\n",
    "            self.beta = np.ones(2*self.m)\n",
    "        elif self.post_distrib == \"gaussian\":\n",
    "            self.mu = np.zeros(2*self.m)\n",
    "            self.sigma = np.zeros(2*self.m)\n",
    "        \n",
    "        \n",
    "\n",
    "         \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the parameters of thompson sampling, Not really used in practise, I prefer to define a band new one\n",
    "        \n",
    "        \"\"\"\n",
    "        self.iteration = 0\n",
    "        self.weight_draw = np.zeros(2*m)\n",
    "        self.numplayed = np.zeros(2)\n",
    "        self.playhist = []\n",
    "        self.decision = 1\n",
    "        \n",
    "        if self.n_init > 0:   \n",
    "            self.initialization = True\n",
    "    \n",
    "        if self.random_variable == \"beta\":\n",
    "            self.alpha = np.ones(2*self.m)\n",
    "            self.beta = np.ones(2*self.m)\n",
    "        elif self.random_variable == \"gaussian\":\n",
    "            self.mu = np.zeros(2*self.m)\n",
    "            self.sigma = np.zeros(2*self.m)\n",
    "        \n",
    "        \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        This will drow a sample from posterior distribution using the parameters and return a decision that maximise \n",
    "        the reward function f with those sample\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.iteration < self.n_init:\n",
    "            self.decision = 0\n",
    "            self.playhist.append(self.decision)\n",
    "            return(self.decision)\n",
    "        \n",
    "        if self.iteration < 2*self.n_init:\n",
    "            self.decision = 1\n",
    "            self.playhist.append(self.decision)\n",
    "            return(self.decision)\n",
    "        else:       \n",
    "            self.initialization = False\n",
    "        \n",
    "            \n",
    "        if self.post_distrib == \"beta\":\n",
    "            self.weight_draw = beta.rvs(self.alpha, self.beta)\n",
    "            \n",
    "        elif self.post_distrib == \"gaussian\": \n",
    "            self.weight_draw = norm.rvs(self.mu, self.sigma)\n",
    "                    \n",
    "        mean_a = np.sum(self.weight_draw[:self.m])\n",
    "        mean_b = np.sum(self.weight_draw[self.m:])\n",
    "        if mean_a > mean_b:\n",
    "            self.decision = 0\n",
    "        else :\n",
    "            self.decision = 1\n",
    "            \n",
    "            \n",
    "        self.playhist.append(self.decision)\n",
    "\n",
    "        return self.decision\n",
    "\n",
    "\n",
    "    def update(self, observation):\n",
    "        \"\"\"\n",
    "        Update the parameters of the arms\n",
    "        observation are a dictionary of reward\n",
    "        \"\"\"\n",
    "        self.iteration += 1\n",
    "        self.numplayed[self.decision] += 1\n",
    "        \n",
    "        if self.post_distrib == \"beta\":\n",
    "            if self.decision == 0:    \n",
    "                self.alpha[:self.m] += observation\n",
    "                self.beta[:self.m] += 1-observation\n",
    "            elif self.decision == 1:\n",
    "                self.alpha[self.m:] += observation\n",
    "                self.beta[self.m:] += 1-observation\n",
    "        if self.post_distrib == \"gaussian\":\n",
    "            if self.decision == 0:    \n",
    "                self.mu[:self.m] = (self.mu[:self.m]*(self.numplayed[self.decision]-1)+observation)/self.numplayed[self.decision]\n",
    "                self.sigma[:self.m] =  1/np.sqrt(self.numplayed[self.decision])\n",
    "            elif self.decision == 1:\n",
    "                self.mu[self.m:] = (self.mu[self.m:]*(self.numplayed[self.decision]-1)+observation)/self.numplayed[self.decision]\n",
    "                self.sigma[self.m:] =  1/np.sqrt(self.numplayed[self.decision])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TS First Optimal play \n",
    "\n",
    "Experiment to compute the first time TS play the optimal arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cold Start\n",
    "\n",
    "Without forced exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing the first time optimal is played\n",
    "\n",
    "# Params\n",
    "jump = 10\n",
    "ms = list(range(100,161,jump))\n",
    "n_trial = 1000 \n",
    "a=1\n",
    "b=0.6\n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"a\"] = a\n",
    "Param_dict[\"b\"] = b\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "\n",
    "upper_limit = 20000000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_opti_{}_{}_{}_a{:.3f}b{:.3f}\".format(ms[0],ms[-1],jump,a,b)\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n",
    "\n",
    "distrib_first_optimal = [[] for m in ms]\n",
    "\n",
    "\n",
    "for i,m in enumerate(ms[::-1]):\n",
    "    print(\"m = \",m) \n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "\n",
    "        env = Two_decision_env(m,a = a,b = b)\n",
    "        player = CTS_exp(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "        while player.initialization:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "        \n",
    "        while player.decision and counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "\n",
    "                if counter%2000 == 0:\n",
    "                    print(\"Have still not played the optimal arm at time {}\".format(counter))\n",
    "\n",
    "        distrib_first_optimal[-(i+1)].append(counter)   \n",
    "\n",
    "np.save(experimentfolder +\"\\\\Distribution\",distrib_first_optimal)\n",
    "\n",
    "#plot Result ECDF\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    cdf = ECDF(distrib_first_optimal[i])\n",
    "    plt.plot([0]+list(np.sort(distrib_first_optimal[i])),[0]+list(cdf(np.sort(distrib_first_optimal[i]))),\"+-\",linewidth=3,markersize=10)\n",
    "#plt.title(\"Distribution of first optimal play  for b = {:.1f} and d = {:d} \".format(b,d),fontsize=20)   \n",
    "    plt.xlabel(r'$T_{opti}$', fontsize=20)\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    \n",
    "    plt.savefig(experimentfolder+'\\\\ECDF_m{}_l{}.pdf'.format(m,n_init))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3)\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='b', alpha=.3)\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "plt.ylabel(r'$T_{opti}$', fontsize=20)\n",
    "plt.xlabel('m', fontsize=20)\n",
    "plt.tick_params(axis='x', labelsize=18)\n",
    "plt.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "plt.savefig(experimentfolder +\"\\\\Averagel{}.pdf\".format(n_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Average time for different delta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_100_160_10_a1.000b0.600\\\\Distribution.npy\")[:-2:,::]\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(100,141,10)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3, color='b')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='b', alpha=.3)\n",
    "plt.plot(ms, Average_time, color='b', label = r\"$\\delta = 0.4$\")\n",
    "\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_15_30_5_a1.000b0.700\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(15,31,5)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3, color='g')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='g', alpha=.3)\n",
    "plt.plot(ms, Average_time, color='g', label = r\"$\\delta = 0.3$\")\n",
    "\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_6_14_2_a1.000b0.800\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(6,15,2)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3,color='r')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='r', alpha=.3)\n",
    "plt.plot(ms, Average_time,color='r', label = r\"$\\delta = 0.2$\")\n",
    "\n",
    "\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_4_7_1_a1.000b0.900\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(4,8,1)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3,color='c')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='c', alpha=.3)\n",
    "plt.plot(ms, Average_time, color = 'c', label = r\"$\\delta = 0.1$\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "plt.ylabel(r'$T_{opti}$', fontsize=20)\n",
    "plt.xlabel('m', fontsize=20)\n",
    "plt.tick_params(axis='x', labelsize=18)\n",
    "plt.tick_params(axis='y', labelsize=18)\n",
    "plt.legend(fontsize = 18)\n",
    "\n",
    "plt.savefig(\"First_playing time_Average_different_delta_2decision.pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Warm Start\n",
    "\n",
    "With forced exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing the first time optimal is played\n",
    "\n",
    "# Params\n",
    "jump = 2\n",
    "ms = list(range(2,12,jump))\n",
    "n_trial = 1000 \n",
    "a=1\n",
    "b=0.99\n",
    "n_init = 3\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"a\"] = a\n",
    "Param_dict[\"b\"] = b\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "Param_dict[\"n_init\"] = n_init\n",
    "Param_dict[\"varl\"] = 1\n",
    "Param_dict[\"vard\"] = 0\n",
    "\n",
    "upper_limit = 5000000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_opti_{}_{}_{}_a{:.5f}b{:.3f}l{}varl{}vardelta{}\".format(ms[0],ms[-1],jump,a,b,2*n_init,Param_dict[\"varl\"],Param_dict[\"vard\"])\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n",
    "\n",
    "distrib_first_optimal = [[] for m in ms]\n",
    "\n",
    "\n",
    "for i,m in enumerate(ms[::-1]):\n",
    "    print(\"m = \",m)\n",
    "    if Param_dict[\"vard\"]:\n",
    "        b = 1- Param_dict[\"vard\"]/m\n",
    "    if Param_dict[\"varl\"]:\n",
    "        n_init = m*Param_dict[\"varl\"]\n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "\n",
    "        env = Two_decision_env(m,a = a,b = b)\n",
    "        player = CTS_exp(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "        while player.initialization:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "        \n",
    "        while player.decision and counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "\n",
    "                if counter%2000 == 0:\n",
    "                    print(\"Have still not played the optimal arm at time {}\".format(counter))\n",
    "\n",
    "        distrib_first_optimal[-(i+1)].append(counter)   \n",
    "\n",
    "np.save(experimentfolder +\"\\\\Distribution\",distrib_first_optimal)\n",
    "\n",
    "#plot Result ECDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experimentfolder = \"Experience_opti_6_14_2_a1.000b0.800\"\n",
    "distrib_first_optimal = np.load(experimentfolder + \"\\\\Distribution.npy\")\n",
    "ms = range(6,15,2)\n",
    "n_trial = 1000\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "#     if Param_dict[\"varl\"]:\n",
    "#         n_init = Param_dict[\"varl\"]*m\n",
    "#     if Param_dict[\"vard\"]:\n",
    "#         b = 1-Param_dict[\"vard\"]/m\n",
    "    plt.figure(i,figsize = (16,12))\n",
    "    plt.clf()\n",
    "    cdf = ECDF(distrib_first_optimal[i])\n",
    "    plt.plot([0]+list(np.sort(distrib_first_optimal[i])),[0]+list(cdf(np.sort(distrib_first_optimal[i]))),\"+-\",linewidth=3,markersize=10)\n",
    "#plt.title(\"Distribution of first optimal play  for b = {:.1f} and d = {:d} \".format(b,d),fontsize=20)   \n",
    "    plt.xlabel(r'$T_{opti}$', fontsize=50)\n",
    "    plt.tick_params(axis='x', labelsize=50)\n",
    "    plt.tick_params(axis='y', labelsize=50)\n",
    "    \n",
    "    plt.savefig(experimentfolder+'\\\\ECDF_m{}_l{}.pdf'.format(m,n_init))\n",
    "    plt.savefig(experimentfolder+'\\\\ECDF_m{}_l{}'.format(m,n_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# experimentfolder = \"Experience_opti_2_10_2_a1.00000b0.990l6varl1vardelta0\"\n",
    "# distrib_first_optimal = np.load(experimentfolder + \"\\\\Distribution.npy\")\n",
    "# ms = range(2,11,2)\n",
    "# n_trial = 1000\n",
    "\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "plt.errorbar(ms[:-1], Average_time[:-1], std[:-1], linewidth=3)\n",
    "plt.fill_between(ms[:-1],Average_time[:-1] - std[:-1], Average_time[:-1] + std[:-1] , color='b', alpha=.3)\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "# plt.ylabel(r'$T_{opti}$', fontsize=40)\n",
    "plt.xlabel('m', fontsize=40)\n",
    "plt.tick_params(axis='x', labelsize=40)\n",
    "plt.tick_params(axis='y', labelsize=40)\n",
    "\n",
    "plt.savefig(experimentfolder +\"\\\\Averagel{}.pdf\".format(n_init))\n",
    "plt.savefig(experimentfolder +\"\\\\Averagel{}\".format(n_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Exploit those results\n",
    "\n",
    "Provide a nice visualisation of the results, need to manually  input the folders used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "n_trial = 1000\n",
    "\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_100_160_10_a1.000b0.600\\\\Distribution.npy\")[:-2]\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(100,141,10)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3, color='g')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='g', alpha=.3)\n",
    "plt.plot(ms, Average_time, color='g', label = r\"$\\delta = 0.4$\")\n",
    "\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_15_30_5_a1.000b0.700\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(15,31,5)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3,color='r')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='r', alpha=.3)\n",
    "plt.plot(ms, Average_time,color='r', label = r\"$\\delta = 0.3$\")\n",
    "\n",
    "\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_6_14_2_a1.000b0.800\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(6,15,2)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3,color='c')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='c', alpha=.3)\n",
    "plt.plot(ms, Average_time, color = 'c', label = r\"$\\delta = 0.2$\")\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_4_7_1_a1.000b0.900\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(4,8,1)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3,color='b')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='b', alpha=.3)\n",
    "plt.plot(ms, Average_time, color = 'b', label = r\"$\\delta = 0.1$\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "plt.ylabel(r'$T_{opti}$', fontsize=40)\n",
    "plt.xlabel('m', fontsize=40)\n",
    "plt.tick_params(axis='x', labelsize=40)\n",
    "plt.tick_params(axis='y', labelsize=40)\n",
    "plt.legend(fontsize = 40, loc =9)\n",
    "\n",
    "plt.savefig(\"First_playing_time_Average_different_delta_2decision.pdf\") \n",
    "plt.savefig(\"First_playing_time_Average_different_delta_2decision.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regret\n",
    "\n",
    "Simulate some regret trajectories either with Thompson, Thompson warm start or ESCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Params\n",
    "jump = 2\n",
    "ms = list(range(1,12,jump))\n",
    "\n",
    "n_trial = 40\n",
    "upper_limit = 400000\n",
    "a=1\n",
    "b=0.9\n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"a\"] = a\n",
    "Param_dict[\"b\"] = b\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "Param_dict[\"n_init\"] = n_init\n",
    "Param_dict[\"upper_limit\"] = upper_limit\n",
    "Param_dict[\"Vard\"] = 1\n",
    "\n",
    "\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_regret_{}_{}_{}_a{:.3f}b{:.3f}vardelta{}\".format(ms[0],ms[-1],jump,a,b,Param_dict[\"Vard\"])\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## TS Coldstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "regrets = np.zeros((len(ms),n_trial,upper_limit))\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    print(\"m = \",m)\n",
    "    if Param_dict[\"Vard\"]:\n",
    "        b = 1- Param_dict[\"Vard\"]/m\n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "\n",
    "        env = Two_decision_env(m,a = a,b = b,sigma = 1, random_variable = random_variable)\n",
    "        player = CTS_exp(m,n_init = 0, post_distrib = post_distrib)\n",
    "\n",
    "        counter = 0\n",
    "    \n",
    "        \n",
    "        while counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "                \n",
    "\n",
    "                if counter%5000 == 0:\n",
    "                    print(\"play number {}\".format(counter))\n",
    "\n",
    "        regrets[i,iteration,::] = np.array(player.playhist)*m*(a-b)\n",
    "\n",
    "np.save(experimentfolder +\"\\\\TSregrets\",regrets)\n",
    "cumulative_regretsTS = np.cumsum(regrets, axis = 2)\n",
    "        \n",
    "for i,m in enumerate(ms):\n",
    "    if Param_dict[\"Var\"]:\n",
    "        b = 1- 1/m\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    plt.title(\" m = {} , b = {:.4f}\".format(m,b), fontsize=20)\n",
    "    for k in np.random.randint(0,n_trial,20):\n",
    "        plt.plot(cumulative_regretsTS[i,k,::])\n",
    "    plt.plot(np.mean(cumulative_regretsTS[i,::,::],axis = 0), label = \"Mean Regret\",linewidth=5)\n",
    "    maxi95 = np.argsort(cumulative_regretsTS[i,::,-1])[int(n_trial*0.975)]\n",
    "    maxi95 = cumulative_regretsTS[i,maxi95,::]\n",
    "    \n",
    "    mini95 = np.argsort(cumulative_regretsTS[i,::,-1])[int(n_trial*0.025)]\n",
    "    mini95 = cumulative_regretsTS[i,mini95,::]\n",
    "    \n",
    "    plt.fill_between(range(upper_limit),mini95,maxi95, color='b', alpha=.1)\n",
    "    \n",
    "    plt.xlabel('t', fontsize=18)\n",
    "    plt.ylabel('Regret', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(experimentfolder+'\\\\TS_regret_m{}_l{}.pdf'.format(m,0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## TS Warm Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "regrets = np.zeros((len(ds),n_trial,upper_limit))\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    print(\"m = \",m)\n",
    "    n_init = 2*m\n",
    "    \n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "\n",
    "        env = Two_decision_env(m,a = a,b = b,sigma = 1, random_variable = random_variable)\n",
    "        player = CTS_exp(m,n_init = 0, post_distrib = post_distrib)\n",
    "\n",
    "        counter = 0\n",
    "    \n",
    "        \n",
    "        while counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "                \n",
    "\n",
    "                if counter%5000 == 0:\n",
    "                    print(\"play number {}\".format(counter))\n",
    "\n",
    "        regrets[i,iteration,::] = np.array(player.playhist)*m*(a-b)\n",
    "\n",
    "        \n",
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    plt.title(\" m = {} , b = {:.4f}\".format(m,b), fontsize=20)\n",
    "    for k in np.random.randint(0,n_trial,20):\n",
    "        plt.plot(cumulative_regrets[i,k,::])\n",
    "    plt.plot(np.mean(cumulative_regrets[i,::,::],axis = 0), label = \"Mean Regret\",linewidth=5)\n",
    "    maxi95 = np.argsort(cumulative_regrets[i,::,-1])[int(n_trial*0.975)]\n",
    "    maxi95 = cumulative_regrets[i,maxi95,::]\n",
    "    \n",
    "    mini95 = np.argsort(cumulative_regrets[i,::,-1])[int(n_trial*0.025)]\n",
    "    mini95 = cumulative_regrets[i,mini95,::]\n",
    "    \n",
    "    plt.fill_between(range(upper_limit),mini95,maxi95, color='b', alpha=.1)\n",
    "    \n",
    "    plt.xlabel('t', fontsize=18)\n",
    "    plt.ylabel('Regret', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(experimentfolder+'\\\\TS_regret_m{}_l{}.pdf'.format(m,2*m))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## ESCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "regrets = np.zeros((len(ms),n_trial,upper_limit))\n",
    "\n",
    "\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    print(\"m = \",m)\n",
    "    if Param_dict[\"Vard\"]:\n",
    "        b = 1- 1/m\n",
    "    \n",
    "    for iteration in range(int(n_trial/10)):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "\n",
    "        env = Two_decision_env(m,a = a,b = b,sigma = 1, random_variable = random_variable)\n",
    "        player = ESCB(m,n_init = 1,index = 2)\n",
    "\n",
    "        counter = 0\n",
    "    \n",
    "        \n",
    "        while counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "                \n",
    "                if counter%5000 == 0:\n",
    "                    print(\"play number {}\".format(counter))\n",
    "\n",
    "        regrets[i,iteration,::] = np.array(player.playhist)*m*(a-b)\n",
    "\n",
    "np.save(experimentfolder +\"\\\\ESCBregrets\",regrets)\n",
    "cumulative_regretsESCB = np.cumsum(regrets, axis = 2)\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    if Param_dict[\"Vard\"]:\n",
    "        b = 1- 1/m\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    plt.title(\" m = {} , b = {:.4f}\".format(m,b), fontsize=20)\n",
    "    for k in np.random.randint(0,n_trial,20):\n",
    "        plt.plot(cumulative_regretsESCB[i,k,::])\n",
    "    plt.plot(np.mean(cumulative_regretsESCB[i,::,::],axis = 0), label = \"Mean Regret\",linewidth=5)\n",
    "    maxi95 = np.argsort(cumulative_regretsESCB[i,::,-1])[int(n_trial*0.975)]\n",
    "    maxi95 = cumulative_regretsESCB[i,maxi95,::]\n",
    "    \n",
    "    mini95 = np.argsort(cumulative_regretsESCB[i,::,-1])[int(n_trial*0.025)]\n",
    "    mini95 = cumulative_regretsESCB[i,mini95,::]\n",
    "    \n",
    "    plt.fill_between(range(upper_limit),mini95,maxi95, color='b', alpha=.1)\n",
    "    \n",
    "    plt.xlabel('t', fontsize=18)\n",
    "    plt.ylabel('Regret', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(experimentfolder+'\\\\ESCB_regret_m{}.pdf'.format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Merge result\n",
    "Provide a nice visualisation of the results, need to input manually the folders used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experimentfolder = \"Experience_regret_1_11_2_a1.000b0.900var1\"\n",
    "n_trial = 40\n",
    "\n",
    "regrets = np.load(experimentfolder+\"\\\\ESCBregrets.npy\")\n",
    "cumulative_regretsESCB = np.cumsum(regrets, axis = 2)\n",
    "RegretESCB = np.mean(cumulative_regretsESCB[::,::,-1],axis = 1)\n",
    "stdESCB = np.std(cumulative_regretsESCB[::,::,-1], axis =1)/np.sqrt(n_trial/10)\n",
    "\n",
    "regrets = np.load(experimentfolder+\"\\\\TSregrets.npy\")\n",
    "cumulative_regretsTS = np.cumsum(regrets, axis = 2)\n",
    "RegretTS = np.mean(cumulative_regretsTS[::,::,-1],axis = 1)\n",
    "stdTS = np.std(cumulative_regretsTS[::,::,-1], axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "\n",
    "ms = range(1,12,2)\n",
    "\n",
    "plt.figure(\"Final regret\",figsize = (16,9))\n",
    "plt.clf()\n",
    "\n",
    "plt.errorbar(ms, RegretESCB, stdESCB, linewidth=1)\n",
    "plt.plot(ms,RegretESCB, label = \"ESCB\",linewidth=5,color='b')\n",
    "plt.fill_between(ms,RegretESCB - stdESCB, RegretESCB + stdESCB , color='b', alpha=.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.errorbar(ms, RegretTS, stdTS, linewidth=1,color='r')\n",
    "plt.plot(ms,RegretTS, label = \"TS\",linewidth=5,color='r')\n",
    "plt.fill_between(ms,RegretTS - stdTS, RegretTS + stdTS , color='r', alpha=.2)\n",
    "\n",
    "\n",
    "plt.xlabel('m', fontsize=40)\n",
    "#plt.ylabel('Regret', fontsize=40)\n",
    "plt.tick_params(axis='x', labelsize=40)\n",
    "plt.tick_params(axis='y', labelsize=40)\n",
    "plt.legend(fontsize = 40)\n",
    "plt.savefig(experimentfolder+'\\\\final_regret_comparison_m{}_{}.pdf'.format(ms[0],ms[-1]))\n",
    "plt.savefig(experimentfolder+'\\\\final_regret_comparison_m{}_{}'.format(ms[0],ms[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
