{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# import the package needed to solve the environnement\n",
    "from scipy.optimize import linear_sum_assignment #hungarian matching algorithm\n",
    "import numpy as np\n",
    "from scipy.stats import beta, bernoulli,norm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from statsmodels.distributions.empirical_distribution import ECDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Z GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the bipartite graph matching environnement\n",
    "class zgraph():\n",
    "    \"\"\"\n",
    "    create the bp graph used in the experiment. Can be improved to receive graph parameter if needed\n",
    "    n is the number of vertex of the first groupe\n",
    "    \n",
    "    sample from a random variable of law bernoulli, or gaussian \n",
    "    \n",
    "    \"\"\" \n",
    "    def __init__(self,m,random_variable = \"bernoulli\",delta = 0.9):\n",
    "        self.d = m**2\n",
    "        self.m = m\n",
    "        self.random_variable = random_variable\n",
    "        self.delta = delta\n",
    "        \n",
    "        #adjacency matrix\n",
    "        self.adjacency_matrix = np.eye(m)+ np.eye(m,k=1)\n",
    "        self.adjacency_matrix[m-1,0] = 1\n",
    "        \n",
    "        self.weight_matrix = np.eye(m)+ np.eye(m,k=1)*(1-delta)\n",
    "        self.weight_matrix[m-1,0] = 1-delta\n",
    "                   \n",
    "    def draw(self, arm_played):\n",
    "        \"\"\"\n",
    "        Draw a vector of reward in a dict\n",
    "        Also return the regret of that choice\n",
    "        \n",
    "        \"\"\"\n",
    "        reward_dict = dict()\n",
    "        regret = self.m\n",
    "        # we assume for the moment that all arm are independent\n",
    "        for i in arm_played:\n",
    "            if self.adjacency_matrix[tuple(i)] == 1:\n",
    "                if self.random_variable == \"bernoulli\":\n",
    "                    reward_dict[tuple(i)] = bernoulli.rvs(self.weight_matrix[tuple(i)])\n",
    "                elif self.random_variable == \"gaussian\":\n",
    "                    reward_dict[tuple(i)] = norm.rvs(self.weight_matrix[tuple(i)])\n",
    "\n",
    "\n",
    "                regret -= self.weight_matrix[tuple(i)]\n",
    "            \n",
    "        return reward_dict, regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class zCThompson_sampling():\n",
    "    \"\"\"\n",
    "    Only for the Z graph bipartite matching\n",
    "    \"\"\"\n",
    "    def __init__(self, m, random_variable = \"beta\",n_init = 0,weight_matrix = None):\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.m = m\n",
    "        self.d = m**2\n",
    "        self.weight_matrix = weight_matrix\n",
    "        \n",
    "        \n",
    "        self.random_variable = random_variable\n",
    "        self.post_param = dict()\n",
    "        self.playhist = []\n",
    "        self.number_item_played = dict()\n",
    "        self.initialization = False\n",
    "        \n",
    "        self.n_init = n_init\n",
    "        if self.n_init > 0:   \n",
    "            self.initialization = True\n",
    "        \n",
    "        self.iteration = 0\n",
    "        # Construct the dictionnary of parameter (could store all the parameter in a matrix n*n*2) \n",
    "        # but here for being more modulable it will be on \n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                if j == 1+i or j==i: \n",
    "                    if self.random_variable == \"beta\":\n",
    "                        self.post_param[i,j] = np.array([1.,1.]) # prior is 1,1 for the beta law of each arm\n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.post_param[i,j] = np.array([0.,0.])\n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                elif i==self.m-1 and j==0:\n",
    "                    if self.random_variable == \"beta\":\n",
    "                        self.post_param[i,j] = np.array([1.,1.]) # prior is 1,1 for the beta law of each arm\n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.post_param[i,j] = np.array([0.,0.])\n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                else:     \n",
    "                    self.post_param[i,j] = [False]\n",
    "                    self.number_item_played[i,j] = [False]\n",
    "                    \n",
    "                self.number_item_played[i,j] = 0.\n",
    "        \n",
    "        \n",
    "        self.weight_draw = np.zeros((m,m))\n",
    "         \n",
    "    def reset(self):\n",
    "        self.weight_draw = np.zeros((self.m,self.m))\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if j == 1+i or j == i: \n",
    "                    if self.random_variable == \"beta\":\n",
    "                        self.post_param[i,j] = np.array([1.,1.]) # prior is 1,1 for the beta law of each arm\n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.post_param[i,j] = np.array([0.,0.])\n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                elif j==0 and i==self.m-1:\n",
    "                    if self.random_variable == \"beta\":\n",
    "                        self.post_param[i,j] = np.array([1.,1.]) # prior is 1,1 for the beta law of each arm\n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.post_param[i,j] = np.array([0.,0.])\n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                else:     \n",
    "                    self.post_param[i,j] = [False]\n",
    "                    self.number_item_played[i,j] = [False]\n",
    "                    \n",
    "                self.number_item_played[i,j] = 0.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        This will draw a sample (arm played) using the parameters \n",
    "\n",
    "        \"\"\"\n",
    "        self.iteration +=1\n",
    "        self.weight_draw = np.zeros((self.m,self.m))\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if self.post_param[(i,j)][0]:\n",
    "                    if self.random_variable == \"beta\":\n",
    "                            self.weight_draw[i,j] = beta.rvs(self.post_param[(i,j)][0],self.post_param[(i,j)][1])\n",
    "                            \n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.weight_draw[i,j] = norm.rvs(self.post_param[(i,j)][0],self.post_param[(i,j)][1])\n",
    "                    \n",
    "\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(-self.weight_draw) \n",
    "        # we have to maximize the reward (the scypy implementation minimizes the cast)\n",
    "\n",
    "        arm_played = np.concatenate([row_ind[:,None], col_ind[:,None]],axis=1)\n",
    "        #Trick to have the list of the arm played\n",
    "        \n",
    "        self.playhist.append(arm_played)\n",
    "\n",
    "        return arm_played\n",
    "\n",
    "\n",
    "    def update(self, observation):\n",
    "        \"\"\"\n",
    "        update the parameter of the arms\n",
    "        observation are a dictionary of reward\n",
    "        \"\"\"\n",
    "        for key, valu in observation.items():\n",
    "            self.number_item_played[key] += 1\n",
    "            if  self.post_param[key][0]:\n",
    "                if self.random_variable == \"beta\":\n",
    "                    y = bernoulli.rvs(valu)\n",
    "                    self.post_param[key][0] += y  #alpha update\n",
    "                    self.post_param[key][1] += 1-y  #beta update\n",
    "                elif self.random_variable == \"gaussian\": \n",
    "                    self.post_param[key][0] = (self.post_param[key][0]*(self.number_item_played[key]-1)+valu)/self.number_item_played[key]\n",
    "                    self.post_param[key][1] = 1/np.sqrt(self.number_item_played[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CUCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class zCUCB():\n",
    "    \"\"\"\n",
    "    Only for the bipartite matching (for the moment)\n",
    "    \"\"\"\n",
    "    def __init__(self, m,n_init = 0, eps = 10^-8):\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.m = m\n",
    "        self.d = m**2\n",
    "        self.weight_matrix = weight_matrix\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.initialization = False\n",
    "        \n",
    "        self.random_variable = random_variable\n",
    "        self.post_param = dict()\n",
    "        self.playhist = []\n",
    "        self.number_item_played = dict()\n",
    "        \n",
    "        self.n_init = n_init\n",
    "        if self.n_init > 0:   \n",
    "            self.initialization = True\n",
    "        \n",
    "        self.iteration = 0\n",
    "        # Construct the dictionnary of parameter (could store all the parameter in a matrix n*n*2) \n",
    "        # but here for being more modulable it will be on \n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if j == 1+i or j == i: \n",
    "                    self.mu[i,j] = np.array([self.eps]) \n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                elif j==0 and i==self.m-1:\n",
    "                    self.mu[i,j] = np.array([self.eps]) \n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                else:     \n",
    "                    self.mu[i,j] = [False]\n",
    "                    self.number_item_played[i,j] = [False]\n",
    "                    \n",
    "        \n",
    "         \n",
    "    def reset(self):\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if j == 1+i or j == i: \n",
    "                    self.mu[i,j] = np.array([self.eps]) \n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                elif j==0 and i==self.m-1:\n",
    "                    self.mu[i,j] = np.array([self.eps]) \n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                else:     \n",
    "                    self.mu[i,j] = [False]\n",
    "                    self.number_item_played[i,j] = [False]\n",
    "                    \n",
    "                self.number_item_played[i,j] = 0.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        This will choose the arm according to the bonus index of CUCB\n",
    "\n",
    "        \"\"\"\n",
    "        self.iteration +=1\n",
    "        self.bonus_index = np.zeros((self.m,self.m))\n",
    "        \n",
    "        if self.iteration == 1:\n",
    "            for i in range(self.m):\n",
    "                for j in range(self.m):\n",
    "                    if j == 1+i or j == i: \n",
    "                        self.bonus_index[i,j] = np.inf\n",
    "                    elif j==0 and i==self.m-1:\n",
    "                        self.bonus_index[i,j] = np.inf\n",
    "        else :\n",
    "            for i in range(self.m):\n",
    "                for j in range(self.m):\n",
    "                    if j == 1+i or j == i: \n",
    "                        self.bonus_index[i,j] = self.mu[i,j][0] + np.sqrt(1.5*np.log(self.iteration)/self.number_item_played[i,j])\n",
    "                    elif j==0 and i==self.m-1:\n",
    "                        self.bonus_index[i,j] = self.mu[i,j][0] + np.sqrt(1.5*np.log(self.iteration)/self.number_item_played[i,j])\n",
    "        \n",
    "                \n",
    "        \n",
    "        row_ind, col_ind = linear_sum_assignment(-self.bonus_index) \n",
    "        # we have to maximize the reward (the scypy implementation minimizes the cast)\n",
    "\n",
    "        arm_played = np.concatenate([row_ind[:,None], col_ind[:,None]],axis=1)\n",
    "        #Trick to have the list of the arm played\n",
    "        \n",
    "        self.playhist.append(arm_played)\n",
    "\n",
    "        return arm_played\n",
    "\n",
    "\n",
    "    def update(self, observation):\n",
    "        \"\"\"\n",
    "        update the parameter of the arms\n",
    "        observation are a dictionary of reward\n",
    "        \"\"\"\n",
    "        \n",
    "        for key, valu in observation.items():\n",
    "            self.number_item_played[key] += 1\n",
    "            if  self.mu[key][0]:\n",
    "                    self.mu[key] = (self.mu[key]*(self.iteration[key]-1)+valu)/self.iteration[key]\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS First Optimal play "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  11\n",
      "begin_trial 0\n",
      "Have still not played the optimal arm at time 2000\n",
      "Have still not played the optimal arm at time 4000\n",
      "Have still not played the optimal arm at time 6000\n",
      "Have still not played the optimal arm at time 8000\n",
      "Have still not played the optimal arm at time 10000\n",
      "Have still not played the optimal arm at time 12000\n",
      "Have still not played the optimal arm at time 14000\n",
      "Have still not played the optimal arm at time 16000\n",
      "Have still not played the optimal arm at time 18000\n",
      "Have still not played the optimal arm at time 20000\n",
      "Have still not played the optimal arm at time 22000\n",
      "Have still not played the optimal arm at time 24000\n",
      "Have still not played the optimal arm at time 26000\n",
      "Have still not played the optimal arm at time 28000\n",
      "Have still not played the optimal arm at time 30000\n",
      "Have still not played the optimal arm at time 32000\n",
      "Have still not played the optimal arm at time 34000\n",
      "Have still not played the optimal arm at time 36000\n",
      "Have still not played the optimal arm at time 38000\n",
      "Have still not played the optimal arm at time 40000\n",
      "Have still not played the optimal arm at time 42000\n",
      "Have still not played the optimal arm at time 44000\n",
      "Have still not played the optimal arm at time 46000\n",
      "Have still not played the optimal arm at time 48000\n",
      "Have still not played the optimal arm at time 50000\n",
      "Have still not played the optimal arm at time 52000\n",
      "Have still not played the optimal arm at time 54000\n",
      "Have still not played the optimal arm at time 56000\n",
      "Have still not played the optimal arm at time 58000\n",
      "Have still not played the optimal arm at time 60000\n",
      "Have still not played the optimal arm at time 62000\n",
      "Have still not played the optimal arm at time 64000\n",
      "Have still not played the optimal arm at time 66000\n",
      "Have still not played the optimal arm at time 68000\n",
      "Have still not played the optimal arm at time 70000\n",
      "Have still not played the optimal arm at time 72000\n",
      "Have still not played the optimal arm at time 2000\n",
      "Have still not played the optimal arm at time 4000\n",
      "Have still not played the optimal arm at time 6000\n",
      "Have still not played the optimal arm at time 8000\n",
      "Have still not played the optimal arm at time 10000\n",
      "Have still not played the optimal arm at time 12000\n",
      "Have still not played the optimal arm at time 14000\n",
      "Have still not played the optimal arm at time 16000\n",
      "Have still not played the optimal arm at time 18000\n",
      "Have still not played the optimal arm at time 20000\n",
      "Have still not played the optimal arm at time 22000\n",
      "Have still not played the optimal arm at time 24000\n",
      "Have still not played the optimal arm at time 26000\n",
      "Have still not played the optimal arm at time 28000\n",
      "Have still not played the optimal arm at time 30000\n",
      "Have still not played the optimal arm at time 32000\n",
      "Have still not played the optimal arm at time 34000\n",
      "Have still not played the optimal arm at time 2000\n",
      "Have still not played the optimal arm at time 4000\n",
      "Have still not played the optimal arm at time 6000\n",
      "Have still not played the optimal arm at time 8000\n",
      "Have still not played the optimal arm at time 10000\n",
      "Have still not played the optimal arm at time 12000\n",
      "Have still not played the optimal arm at time 14000\n",
      "Have still not played the optimal arm at time 16000\n",
      "Have still not played the optimal arm at time 18000\n",
      "Have still not played the optimal arm at time 20000\n",
      "Have still not played the optimal arm at time 22000\n",
      "Have still not played the optimal arm at time 24000\n",
      "Have still not played the optimal arm at time 26000\n",
      "Have still not played the optimal arm at time 28000\n",
      "Have still not played the optimal arm at time 30000\n",
      "Have still not played the optimal arm at time 32000\n",
      "Have still not played the optimal arm at time 34000\n",
      "Have still not played the optimal arm at time 36000\n",
      "Have still not played the optimal arm at time 38000\n",
      "Have still not played the optimal arm at time 40000\n",
      "Have still not played the optimal arm at time 2000\n",
      "Have still not played the optimal arm at time 2000\n",
      "Have still not played the optimal arm at time 2000\n",
      "Have still not played the optimal arm at time 4000\n",
      "Have still not played the optimal arm at time 6000\n",
      "Have still not played the optimal arm at time 8000\n",
      "Have still not played the optimal arm at time 10000\n",
      "Have still not played the optimal arm at time 12000\n",
      "Have still not played the optimal arm at time 14000\n",
      "Have still not played the optimal arm at time 16000\n",
      "Have still not played the optimal arm at time 18000\n",
      "Have still not played the optimal arm at time 20000\n",
      "Have still not played the optimal arm at time 22000\n",
      "Have still not played the optimal arm at time 24000\n",
      "Have still not played the optimal arm at time 26000\n",
      "Have still not played the optimal arm at time 28000\n",
      "Have still not played the optimal arm at time 30000\n",
      "Have still not played the optimal arm at time 32000\n",
      "Have still not played the optimal arm at time 34000\n",
      "Have still not played the optimal arm at time 36000\n",
      "Have still not played the optimal arm at time 38000\n",
      "Have still not played the optimal arm at time 40000\n",
      "Have still not played the optimal arm at time 42000\n",
      "Have still not played the optimal arm at time 44000\n",
      "Have still not played the optimal arm at time 46000\n",
      "Have still not played the optimal arm at time 48000\n",
      "Have still not played the optimal arm at time 50000\n",
      "Have still not played the optimal arm at time 52000\n",
      "Have still not played the optimal arm at time 54000\n",
      "Have still not played the optimal arm at time 56000\n",
      "Have still not played the optimal arm at time 58000\n",
      "Have still not played the optimal arm at time 60000\n",
      "Have still not played the optimal arm at time 62000\n",
      "Have still not played the optimal arm at time 64000\n",
      "Have still not played the optimal arm at time 66000\n",
      "Have still not played the optimal arm at time 68000\n",
      "Have still not played the optimal arm at time 70000\n",
      "Have still not played the optimal arm at time 72000\n",
      "Have still not played the optimal arm at time 74000\n",
      "Have still not played the optimal arm at time 76000\n",
      "Have still not played the optimal arm at time 78000\n",
      "Have still not played the optimal arm at time 80000\n",
      "Have still not played the optimal arm at time 82000\n",
      "Have still not played the optimal arm at time 84000\n",
      "Have still not played the optimal arm at time 86000\n",
      "Have still not played the optimal arm at time 88000\n",
      "Have still not played the optimal arm at time 90000\n",
      "Have still not played the optimal arm at time 92000\n",
      "Have still not played the optimal arm at time 94000\n",
      "Have still not played the optimal arm at time 96000\n",
      "Have still not played the optimal arm at time 98000\n",
      "Have still not played the optimal arm at time 100000\n",
      "Have still not played the optimal arm at time 102000\n",
      "Have still not played the optimal arm at time 104000\n",
      "Have still not played the optimal arm at time 106000\n",
      "Have still not played the optimal arm at time 108000\n",
      "Have still not played the optimal arm at time 110000\n",
      "Have still not played the optimal arm at time 112000\n",
      "Have still not played the optimal arm at time 114000\n",
      "Have still not played the optimal arm at time 116000\n",
      "Have still not played the optimal arm at time 118000\n",
      "Have still not played the optimal arm at time 120000\n",
      "Have still not played the optimal arm at time 122000\n",
      "Have still not played the optimal arm at time 124000\n",
      "Have still not played the optimal arm at time 126000\n",
      "Have still not played the optimal arm at time 128000\n",
      "Have still not played the optimal arm at time 130000\n",
      "Have still not played the optimal arm at time 132000\n",
      "Have still not played the optimal arm at time 134000\n",
      "Have still not played the optimal arm at time 136000\n",
      "Have still not played the optimal arm at time 138000\n",
      "Have still not played the optimal arm at time 140000\n",
      "Have still not played the optimal arm at time 142000\n",
      "Have still not played the optimal arm at time 144000\n",
      "Have still not played the optimal arm at time 146000\n",
      "Have still not played the optimal arm at time 148000\n",
      "Have still not played the optimal arm at time 150000\n",
      "Have still not played the optimal arm at time 152000\n",
      "Have still not played the optimal arm at time 154000\n",
      "Have still not played the optimal arm at time 156000\n",
      "Have still not played the optimal arm at time 158000\n",
      "Have still not played the optimal arm at time 160000\n",
      "Have still not played the optimal arm at time 162000\n",
      "Have still not played the optimal arm at time 164000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have still not played the optimal arm at time 166000\n",
      "Have still not played the optimal arm at time 168000\n",
      "Have still not played the optimal arm at time 170000\n",
      "Have still not played the optimal arm at time 172000\n",
      "Have still not played the optimal arm at time 174000\n",
      "Have still not played the optimal arm at time 176000\n",
      "Have still not played the optimal arm at time 178000\n",
      "Have still not played the optimal arm at time 180000\n",
      "Have still not played the optimal arm at time 182000\n",
      "Have still not played the optimal arm at time 184000\n",
      "Have still not played the optimal arm at time 186000\n",
      "Have still not played the optimal arm at time 188000\n",
      "Have still not played the optimal arm at time 190000\n",
      "Have still not played the optimal arm at time 192000\n",
      "Have still not played the optimal arm at time 194000\n",
      "Have still not played the optimal arm at time 196000\n",
      "Have still not played the optimal arm at time 198000\n",
      "Have still not played the optimal arm at time 200000\n",
      "Have still not played the optimal arm at time 202000\n",
      "Have still not played the optimal arm at time 204000\n",
      "Have still not played the optimal arm at time 206000\n",
      "Have still not played the optimal arm at time 208000\n",
      "Have still not played the optimal arm at time 210000\n",
      "Have still not played the optimal arm at time 212000\n",
      "Have still not played the optimal arm at time 214000\n",
      "Have still not played the optimal arm at time 216000\n",
      "Have still not played the optimal arm at time 218000\n",
      "Have still not played the optimal arm at time 220000\n",
      "Have still not played the optimal arm at time 222000\n",
      "Have still not played the optimal arm at time 224000\n",
      "Have still not played the optimal arm at time 226000\n",
      "Have still not played the optimal arm at time 228000\n",
      "Have still not played the optimal arm at time 230000\n",
      "Have still not played the optimal arm at time 232000\n",
      "Have still not played the optimal arm at time 234000\n",
      "Have still not played the optimal arm at time 236000\n",
      "Have still not played the optimal arm at time 238000\n",
      "Have still not played the optimal arm at time 240000\n",
      "Have still not played the optimal arm at time 242000\n",
      "Have still not played the optimal arm at time 244000\n",
      "Have still not played the optimal arm at time 246000\n",
      "Have still not played the optimal arm at time 248000\n",
      "Have still not played the optimal arm at time 250000\n",
      "Have still not played the optimal arm at time 252000\n",
      "Have still not played the optimal arm at time 254000\n",
      "Have still not played the optimal arm at time 256000\n",
      "Have still not played the optimal arm at time 258000\n",
      "Have still not played the optimal arm at time 260000\n",
      "Have still not played the optimal arm at time 262000\n",
      "Have still not played the optimal arm at time 264000\n",
      "Have still not played the optimal arm at time 266000\n",
      "Have still not played the optimal arm at time 268000\n",
      "Have still not played the optimal arm at time 270000\n",
      "Have still not played the optimal arm at time 272000\n",
      "Have still not played the optimal arm at time 274000\n",
      "Have still not played the optimal arm at time 276000\n",
      "Have still not played the optimal arm at time 278000\n",
      "Have still not played the optimal arm at time 280000\n",
      "Have still not played the optimal arm at time 282000\n",
      "Have still not played the optimal arm at time 284000\n",
      "Have still not played the optimal arm at time 286000\n",
      "Have still not played the optimal arm at time 288000\n",
      "Have still not played the optimal arm at time 290000\n",
      "Have still not played the optimal arm at time 292000\n",
      "Have still not played the optimal arm at time 294000\n",
      "Have still not played the optimal arm at time 296000\n",
      "Have still not played the optimal arm at time 298000\n",
      "Have still not played the optimal arm at time 300000\n",
      "Have still not played the optimal arm at time 302000\n",
      "Have still not played the optimal arm at time 304000\n",
      "Have still not played the optimal arm at time 306000\n",
      "Have still not played the optimal arm at time 308000\n",
      "Have still not played the optimal arm at time 310000\n",
      "Have still not played the optimal arm at time 312000\n",
      "Have still not played the optimal arm at time 314000\n",
      "Have still not played the optimal arm at time 316000\n",
      "Have still not played the optimal arm at time 318000\n",
      "Have still not played the optimal arm at time 320000\n",
      "Have still not played the optimal arm at time 322000\n",
      "Have still not played the optimal arm at time 324000\n",
      "Have still not played the optimal arm at time 326000\n",
      "Have still not played the optimal arm at time 328000\n",
      "Have still not played the optimal arm at time 330000\n",
      "Have still not played the optimal arm at time 332000\n",
      "Have still not played the optimal arm at time 334000\n",
      "Have still not played the optimal arm at time 336000\n",
      "Have still not played the optimal arm at time 338000\n",
      "Have still not played the optimal arm at time 340000\n",
      "Have still not played the optimal arm at time 342000\n",
      "Have still not played the optimal arm at time 344000\n",
      "Have still not played the optimal arm at time 346000\n",
      "Have still not played the optimal arm at time 348000\n",
      "Have still not played the optimal arm at time 350000\n",
      "Have still not played the optimal arm at time 352000\n",
      "Have still not played the optimal arm at time 354000\n",
      "Have still not played the optimal arm at time 356000\n",
      "Have still not played the optimal arm at time 358000\n",
      "Have still not played the optimal arm at time 360000\n",
      "Have still not played the optimal arm at time 362000\n",
      "Have still not played the optimal arm at time 364000\n",
      "Have still not played the optimal arm at time 366000\n",
      "Have still not played the optimal arm at time 368000\n",
      "Have still not played the optimal arm at time 370000\n",
      "Have still not played the optimal arm at time 372000\n",
      "Have still not played the optimal arm at time 374000\n",
      "Have still not played the optimal arm at time 376000\n",
      "Have still not played the optimal arm at time 378000\n",
      "Have still not played the optimal arm at time 380000\n",
      "Have still not played the optimal arm at time 382000\n",
      "Have still not played the optimal arm at time 384000\n",
      "Have still not played the optimal arm at time 386000\n",
      "Have still not played the optimal arm at time 388000\n",
      "Have still not played the optimal arm at time 390000\n",
      "Have still not played the optimal arm at time 392000\n",
      "Have still not played the optimal arm at time 394000\n",
      "Have still not played the optimal arm at time 396000\n",
      "Have still not played the optimal arm at time 398000\n",
      "Have still not played the optimal arm at time 400000\n",
      "Have still not played the optimal arm at time 402000\n",
      "Have still not played the optimal arm at time 404000\n",
      "Have still not played the optimal arm at time 406000\n",
      "Have still not played the optimal arm at time 408000\n",
      "Have still not played the optimal arm at time 410000\n",
      "Have still not played the optimal arm at time 412000\n",
      "Have still not played the optimal arm at time 414000\n",
      "Have still not played the optimal arm at time 416000\n",
      "Have still not played the optimal arm at time 418000\n",
      "Have still not played the optimal arm at time 420000\n",
      "Have still not played the optimal arm at time 422000\n",
      "Have still not played the optimal arm at time 424000\n",
      "Have still not played the optimal arm at time 426000\n",
      "Have still not played the optimal arm at time 428000\n",
      "Have still not played the optimal arm at time 430000\n",
      "Have still not played the optimal arm at time 432000\n",
      "Have still not played the optimal arm at time 434000\n",
      "Have still not played the optimal arm at time 436000\n",
      "Have still not played the optimal arm at time 438000\n",
      "Have still not played the optimal arm at time 440000\n",
      "Have still not played the optimal arm at time 442000\n",
      "Have still not played the optimal arm at time 444000\n",
      "Have still not played the optimal arm at time 446000\n",
      "Have still not played the optimal arm at time 448000\n",
      "Have still not played the optimal arm at time 450000\n",
      "Have still not played the optimal arm at time 452000\n",
      "Have still not played the optimal arm at time 454000\n",
      "Have still not played the optimal arm at time 456000\n",
      "Have still not played the optimal arm at time 458000\n",
      "Have still not played the optimal arm at time 460000\n",
      "Have still not played the optimal arm at time 462000\n",
      "Have still not played the optimal arm at time 464000\n",
      "Have still not played the optimal arm at time 466000\n",
      "Have still not played the optimal arm at time 468000\n",
      "Have still not played the optimal arm at time 470000\n",
      "Have still not played the optimal arm at time 472000\n",
      "Have still not played the optimal arm at time 474000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have still not played the optimal arm at time 476000\n",
      "Have still not played the optimal arm at time 478000\n",
      "Have still not played the optimal arm at time 480000\n",
      "Have still not played the optimal arm at time 482000\n",
      "Have still not played the optimal arm at time 484000\n",
      "Have still not played the optimal arm at time 486000\n",
      "Have still not played the optimal arm at time 488000\n",
      "Have still not played the optimal arm at time 490000\n",
      "Have still not played the optimal arm at time 492000\n",
      "Have still not played the optimal arm at time 494000\n",
      "Have still not played the optimal arm at time 496000\n",
      "Have still not played the optimal arm at time 498000\n",
      "Have still not played the optimal arm at time 500000\n",
      "Have still not played the optimal arm at time 502000\n",
      "Have still not played the optimal arm at time 504000\n",
      "Have still not played the optimal arm at time 506000\n",
      "Have still not played the optimal arm at time 508000\n",
      "Have still not played the optimal arm at time 510000\n",
      "Have still not played the optimal arm at time 512000\n",
      "Have still not played the optimal arm at time 514000\n",
      "Have still not played the optimal arm at time 516000\n",
      "Have still not played the optimal arm at time 518000\n",
      "Have still not played the optimal arm at time 520000\n",
      "Have still not played the optimal arm at time 522000\n",
      "Have still not played the optimal arm at time 524000\n",
      "Have still not played the optimal arm at time 526000\n",
      "Have still not played the optimal arm at time 528000\n",
      "Have still not played the optimal arm at time 530000\n",
      "Have still not played the optimal arm at time 532000\n",
      "Have still not played the optimal arm at time 534000\n",
      "begin_trial 100\n",
      "Have still not played the optimal arm at time 2000\n",
      "Have still not played the optimal arm at time 4000\n",
      "Have still not played the optimal arm at time 6000\n",
      "Have still not played the optimal arm at time 8000\n",
      "Have still not played the optimal arm at time 10000\n",
      "Have still not played the optimal arm at time 12000\n",
      "Have still not played the optimal arm at time 14000\n",
      "Have still not played the optimal arm at time 16000\n",
      "Have still not played the optimal arm at time 18000\n",
      "Have still not played the optimal arm at time 20000\n",
      "Have still not played the optimal arm at time 22000\n",
      "Have still not played the optimal arm at time 24000\n",
      "Have still not played the optimal arm at time 26000\n",
      "Have still not played the optimal arm at time 28000\n",
      "Have still not played the optimal arm at time 30000\n",
      "Have still not played the optimal arm at time 32000\n",
      "Have still not played the optimal arm at time 34000\n",
      "Have still not played the optimal arm at time 36000\n",
      "Have still not played the optimal arm at time 38000\n",
      "Have still not played the optimal arm at time 40000\n",
      "Have still not played the optimal arm at time 42000\n",
      "Have still not played the optimal arm at time 44000\n",
      "Have still not played the optimal arm at time 46000\n",
      "Have still not played the optimal arm at time 48000\n",
      "Have still not played the optimal arm at time 50000\n",
      "Have still not played the optimal arm at time 52000\n",
      "Have still not played the optimal arm at time 54000\n",
      "Have still not played the optimal arm at time 56000\n",
      "Have still not played the optimal arm at time 58000\n",
      "Have still not played the optimal arm at time 60000\n",
      "Have still not played the optimal arm at time 62000\n",
      "Have still not played the optimal arm at time 2000\n",
      "Have still not played the optimal arm at time 4000\n",
      "Have still not played the optimal arm at time 6000\n",
      "Have still not played the optimal arm at time 8000\n",
      "Have still not played the optimal arm at time 10000\n",
      "Have still not played the optimal arm at time 12000\n",
      "Have still not played the optimal arm at time 14000\n",
      "Have still not played the optimal arm at time 16000\n",
      "Have still not played the optimal arm at time 18000\n",
      "Have still not played the optimal arm at time 20000\n",
      "Have still not played the optimal arm at time 22000\n",
      "Have still not played the optimal arm at time 24000\n",
      "Have still not played the optimal arm at time 26000\n",
      "Have still not played the optimal arm at time 28000\n",
      "Have still not played the optimal arm at time 30000\n",
      "Have still not played the optimal arm at time 32000\n",
      "Have still not played the optimal arm at time 34000\n",
      "Have still not played the optimal arm at time 36000\n",
      "Have still not played the optimal arm at time 38000\n",
      "Have still not played the optimal arm at time 40000\n",
      "Have still not played the optimal arm at time 42000\n",
      "Have still not played the optimal arm at time 44000\n",
      "Have still not played the optimal arm at time 46000\n",
      "Have still not played the optimal arm at time 48000\n",
      "Have still not played the optimal arm at time 50000\n",
      "Have still not played the optimal arm at time 52000\n",
      "Have still not played the optimal arm at time 54000\n",
      "Have still not played the optimal arm at time 56000\n",
      "Have still not played the optimal arm at time 58000\n",
      "Have still not played the optimal arm at time 60000\n",
      "Have still not played the optimal arm at time 62000\n",
      "Have still not played the optimal arm at time 64000\n",
      "Have still not played the optimal arm at time 66000\n",
      "Have still not played the optimal arm at time 68000\n"
     ]
    }
   ],
   "source": [
    "# testing the first time optimal is played\n",
    "\n",
    "# Params\n",
    "jump = 1\n",
    "ms = list(range(4,12,jump))\n",
    "n_trial = 1000 \n",
    "delta = 0.15\n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"delta\"] = delta\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "\n",
    "upper_limit = 20000000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_opti_{}_{}_{}_delta{:.3f}\".format(ms[0],ms[-1],jump,delta)\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n",
    "\n",
    "distrib_first_optimal = [[] for m in ms]\n",
    "\n",
    "\n",
    "for i,m in enumerate(ms[::-1]):\n",
    "    print(\"m = \",m) \n",
    "    optimal_arm = np.concatenate([np.arange(m)[:,None],np.arange(m)[:,None]],axis = 1)\n",
    "    \n",
    "    \n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "        \n",
    "        optimal_arm_played = False\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCThompson_sampling(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "        while player.initialization:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "        \n",
    "        while not optimal_arm_played and counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "\n",
    "                if counter%2000 == 0:\n",
    "                    print(\"Have still not played the optimal arm at time {}\".format(counter))\n",
    "                    \n",
    "                if (decision == optimal_arm).all():\n",
    "                    optimal_arm_played = True\n",
    "\n",
    "        distrib_first_optimal[-(i+1)].append(counter)   \n",
    "\n",
    "np.save(experimentfolder +\"\\\\Distribution\",distrib_first_optimal)\n",
    "\n",
    "#plot Result ECDF\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    cdf = ECDF(distrib_first_optimal[i])\n",
    "    plt.plot([0]+list(np.sort(distrib_first_optimal[i])),[0]+list(cdf(np.sort(distrib_first_optimal[i]))),\"+-\",linewidth=3,markersize=10)\n",
    "#plt.title(\"Distribution of first optimal play  for b = {:.1f} and d = {:d} \".format(b,d),fontsize=20)   \n",
    "    plt.xlabel(r'$T_{opti}$', fontsize=20)\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    \n",
    "    plt.savefig(experimentfolder+'\\\\ECDF_m{}_l{}.pdf'.format(m,n_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3)\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='b', alpha=.3)\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "plt.ylabel(r'$T_{opti}$', fontsize=20)\n",
    "plt.xlabel('m', fontsize=20)\n",
    "plt.tick_params(axis='x', labelsize=18)\n",
    "plt.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "plt.savefig(experimentfolder +\"\\\\Averagel{}.pdf\".format(n_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the first time optimal is played\n",
    "\n",
    "# Params\n",
    "jump = 1\n",
    "ms = list(range(4,8,jump))\n",
    "n_trial = 1000 \n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "\n",
    "upper_limit = 20000000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_opti_{}_{}_{}_Delta0.5\".format(ms[0],ms[-1],jump)\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n",
    "\n",
    "distrib_first_optimal = [[] for m in ms]\n",
    "\n",
    "\n",
    "for i,m in enumerate(ms[::-1]):\n",
    "    print(\"m = \",m)\n",
    "    delta = 1/(2m)\n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCThompson_sampling(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "        while player.initialization:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "        \n",
    "        while player.decision and counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "\n",
    "                if counter%2000 == 0:\n",
    "                    print(\"Have still not played the optimal arm at time {}\".format(counter))\n",
    "\n",
    "        distrib_first_optimal[-(i+1)].append(counter)   \n",
    "\n",
    "np.save(experimentfolder +\"\\\\Distribution\",distrib_first_optimal)\n",
    "\n",
    "#plot Result ECDF\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    cdf = ECDF(distrib_first_optimal[i])\n",
    "    plt.plot([0]+list(np.sort(distrib_first_optimal[i])),[0]+list(cdf(np.sort(distrib_first_optimal[i]))),\"+-\",linewidth=3,markersize=10)\n",
    "#plt.title(\"Distribution of first optimal play  for b = {:.1f} and d = {:d} \".format(b,d),fontsize=20)   \n",
    "    plt.xlabel(r'$T_{opti}$', fontsize=20)\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    \n",
    "    plt.savefig(experimentfolder+'\\\\ECDF_m{}_l{}.pdf'.format(m,n_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3)\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='b', alpha=.3)\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "plt.ylabel(r'$T_{opti}$', fontsize=20)\n",
    "plt.xlabel('m', fontsize=20)\n",
    "plt.tick_params(axis='x', labelsize=18)\n",
    "plt.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "plt.savefig(experimentfolder +\"\\\\Averagel{}.pdf\".format(n_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Params\n",
    "jump = 1\n",
    "ms = list(range(4,12,jump))\n",
    "n_trial = 50 \n",
    "delta = 0.2\n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"delta\"] = delta\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "\n",
    "upper_limit = 5000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_opti_{}_{}_{}_delta{:.3f}\".format(ms[0],ms[-1],jump,delta)\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets = np.zeros((len(ds),n_trial,upper_limit))\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    print(\"m = \",m)\n",
    "    \n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCThompson_sampling(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "    \n",
    "        \n",
    "        while counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                \n",
    "                regrets[i,iteration,counter] = regret\n",
    "                \n",
    "                counter += 1\n",
    "                if counter%5000 == 0:\n",
    "                    print(\"play number {}\".format(counter))\n",
    "        \n",
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    plt.title(\" m = {} , b = {:.4f}\".format(m,b), fontsize=20)\n",
    "    for k in np.random.randint(0,n_trial,20):\n",
    "        plt.plot(cumulative_regrets[i,k,::])\n",
    "    plt.plot(np.mean(cumulative_regrets[i,::,::],axis = 0), label = \"Mean Regret\",linewidth=5)\n",
    "    maxi95 = np.argsort(cumulative_regrets[i,::,-1])[int(n_trial*0.975)]\n",
    "    maxi95 = cumulative_regrets[i,maxi95,::]\n",
    "    \n",
    "    mini95 = np.argsort(cumulative_regrets[i,::,-1])[int(n_trial*0.025)]\n",
    "    mini95 = cumulative_regrets[i,mini95,::]\n",
    "    \n",
    "    plt.fill_between(range(upper_limit),mini95,maxi95, color='b', alpha=.1)\n",
    "    \n",
    "    plt.xlabel('t', fontsize=18)\n",
    "    plt.ylabel('Regret', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(experimentfolder+'\\\\TS_regret_m{}_l{}.pdf'.format(m,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Unfixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "jump = 1\n",
    "ms = list(range(4,12,jump))\n",
    "n_trial = 1000 \n",
    "delta = 0.15\n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"delta\"] = delta\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "\n",
    "upper_limit = 20000000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_opti_{}_{}_{}_delta{:.3f}\".format(ms[0],ms[-1],jump,delta)\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CUCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
