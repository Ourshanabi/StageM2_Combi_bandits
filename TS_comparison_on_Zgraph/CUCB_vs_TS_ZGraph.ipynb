{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contain the experiment and the code used for our paper for the bipartite matching graph environment.\n",
    "\n",
    "It is advised to open it with the collapse heading option as it is quite heavy.\n",
    "\n",
    "The notebook is composed of 5 parts :\n",
    "    -We define the environment\n",
    "    -We define the Thompson sampling algorithm for this environment\n",
    "    -We define the CUCB algorithm for this environment\n",
    "    -We experiment on the first time the optimal decision is played, with our without forced exploration\n",
    "    -We compare the regret between CUCB and Thompson sampling, with or without forced exploration\n",
    "\n",
    "It is not advised to launch the notebook as it is :\n",
    "    -You should change the saving path of the figures\n",
    "    -Reduce the number of experiment, the time horizon, \n",
    "    -Increase the gap (delta)\n",
    "    -Change the range of the number of arms if needed\n",
    "Our parameters have been chosen to reduce the variance by computing more sample path, and using large time horizon. (As we are showing that a method do not work well you could wait a long time before getting any results with those parameters as we did it once for our paper figures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# import the package needed to solve the environnement\n",
    "from scipy.optimize import linear_sum_assignment #hungarian matching algorithm\n",
    "import numpy as np\n",
    "from scipy.stats import beta, bernoulli,norm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from statsmodels.distributions.empirical_distribution import ECDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Z GRAPH\n",
    "\n",
    "We create the bipartite graph environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the bipartite graph matching environnement\n",
    "class zgraph():\n",
    "    \"\"\"\n",
    "    Create the bp graph used in the experiment. Can be improved to receive graph parameter if needed.\n",
    "    \n",
    "    \n",
    "    input :\n",
    "        - m number of edges in one part of the graph\n",
    "        - random_variable = \"bernoulli\", \"gaussian\" distribution of the arm\n",
    "        - delta : gap of the subotpimal arm (their parameter is 1-delta)\n",
    "        (This is not a version where the suboptimal arm can be set in [1/2,1], the reader can make the\n",
    "        change if they want to try)\n",
    "    \"\"\" \n",
    "    def __init__(self,m,random_variable = \"bernoulli\",delta = 0.2):\n",
    "        self.d = m**2\n",
    "        self.m = m\n",
    "        self.random_variable = random_variable\n",
    "        self.delta = delta\n",
    "        \n",
    "        #adjacency matrix\n",
    "        self.adjacency_matrix = np.eye(m)+ np.eye(m,k=1)\n",
    "        self.adjacency_matrix[m-1,0] = 1\n",
    "        \n",
    "        self.weight_matrix = np.eye(m)+ np.eye(m,k=1)*(1-delta)\n",
    "        self.weight_matrix[m-1,0] = 1-delta\n",
    "                   \n",
    "    def draw(self, arm_played):\n",
    "        \"\"\"\n",
    "        Draw a vector of reward in a dict\n",
    "        Also return the regret of that choice\n",
    "        \n",
    "        \"\"\"\n",
    "        reward_dict = dict()\n",
    "        regret = self.m\n",
    "        # we assume for the moment that all arm are independent\n",
    "        for i in arm_played:\n",
    "            if self.adjacency_matrix[tuple(i)] == 1:\n",
    "                if self.random_variable == \"bernoulli\":\n",
    "                    reward_dict[tuple(i)] = bernoulli.rvs(self.weight_matrix[tuple(i)])\n",
    "                elif self.random_variable == \"gaussian\":\n",
    "                    reward_dict[tuple(i)] = norm.rvs(self.weight_matrix[tuple(i)])\n",
    "\n",
    "\n",
    "                regret -= self.weight_matrix[tuple(i)]\n",
    "            \n",
    "        return reward_dict, regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TS\n",
    "\n",
    "Define the TS algorithm specific for the bipartite graph environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class zCThompson_sampling():\n",
    "    \"\"\"\n",
    "    Only for the Z graph bipartite matching\n",
    "    \n",
    "    input :\n",
    "        - m number of edges in one part of the graph\n",
    "        - random_variable = \"beta\", \"gaussian\" use beta for bernoulli environment, \n",
    "        gaussian for gaussian environment\n",
    "        - n_init number of forced exploration set to 0 (not implemented on this environmemt)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, m, random_variable = \"beta\",n_init = 0,weight_matrix = None):\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.m = m\n",
    "        self.d = m**2\n",
    "        self.weight_matrix = weight_matrix\n",
    "        \n",
    "        \n",
    "        self.random_variable = random_variable\n",
    "        self.post_param = dict()\n",
    "        self.playhist = []\n",
    "        self.number_item_played = dict()\n",
    "        self.initialization = False\n",
    "        \n",
    "        self.n_init = n_init\n",
    "        if self.n_init > 0:   \n",
    "            self.initialization = True\n",
    "        \n",
    "        self.iteration = 0\n",
    "        # Construct the dictionnary of parameter (could store all the parameter in a matrix n*n*2) \n",
    "        # but here for being more modulable it will be on a dictionary\n",
    "        for i in range(m):\n",
    "            for j in range(m):\n",
    "                if j == 1+i or j==i: \n",
    "                    if self.random_variable == \"beta\":\n",
    "                        self.post_param[i,j] = np.array([1.,1.]) # prior is 1,1 for the beta law of each arm\n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.post_param[i,j] = np.array([0.,0.])\n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                elif i==self.m-1 and j==0:\n",
    "                    if self.random_variable == \"beta\":\n",
    "                        self.post_param[i,j] = np.array([1.,1.]) # prior is 1,1 for the beta law of each arm\n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.post_param[i,j] = np.array([0.,0.])\n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                else:     \n",
    "                    self.post_param[i,j] = [False]\n",
    "                    self.number_item_played[i,j] = [False]\n",
    "                    \n",
    "                self.number_item_played[i,j] = 0.\n",
    "        \n",
    "        \n",
    "        self.weight_draw = np.zeros((m,m))\n",
    "         \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset the algorithm\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weight_draw = np.zeros((self.m,self.m))\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if j == 1+i or j == i: \n",
    "                    if self.random_variable == \"beta\":\n",
    "                        self.post_param[i,j] = np.array([1.,1.]) # prior is 1,1 for the beta law of each arm\n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.post_param[i,j] = np.array([0.,0.])\n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                elif j==0 and i==self.m-1:\n",
    "                    if self.random_variable == \"beta\":\n",
    "                        self.post_param[i,j] = np.array([1.,1.]) # prior is 1,1 for the beta law of each arm\n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.post_param[i,j] = np.array([0.,0.])\n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                else:     \n",
    "                    self.post_param[i,j] = [False]\n",
    "                    self.number_item_played[i,j] = [False]\n",
    "                    \n",
    "                self.number_item_played[i,j] = 0.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        This will draw a sample (arm played) using the parameters \n",
    "\n",
    "        \"\"\"\n",
    "        self.iteration +=1\n",
    "        self.weight_draw = np.zeros((self.m,self.m))\n",
    "        \n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if self.post_param[(i,j)][0]:\n",
    "                    if self.random_variable == \"beta\":\n",
    "                            self.weight_draw[i,j] = beta.rvs(self.post_param[(i,j)][0],self.post_param[(i,j)][1])\n",
    "                            \n",
    "                    elif self.random_variable == \"gaussian\": \n",
    "                        self.weight_draw[i,j] = norm.rvs(self.post_param[(i,j)][0],self.post_param[(i,j)][1])\n",
    "                    \n",
    "\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(-self.weight_draw) \n",
    "        # we have to maximize the reward (the scypy implementation minimizes the cast)\n",
    "\n",
    "        arm_played = np.concatenate([row_ind[:,None], col_ind[:,None]],axis=1)\n",
    "        #Trick to have the list of the arm played\n",
    "        \n",
    "        self.playhist.append(arm_played)\n",
    "\n",
    "        return arm_played\n",
    "\n",
    "\n",
    "    def update(self, observation):\n",
    "        \"\"\"\n",
    "        update the parameter of the arms\n",
    "        observation is a dictionary of reward\n",
    "        \"\"\"\n",
    "        for key, valu in observation.items():\n",
    "            self.number_item_played[key] += 1\n",
    "            if  self.post_param[key][0]:\n",
    "                if self.random_variable == \"beta\":\n",
    "                    y = bernoulli.rvs(valu)\n",
    "                    self.post_param[key][0] += y  #alpha update\n",
    "                    self.post_param[key][1] += 1-y  #beta update\n",
    "                elif self.random_variable == \"gaussian\": \n",
    "                    self.post_param[key][0] = (self.post_param[key][0]*(self.number_item_played[key]-1)+valu)/self.number_item_played[key]\n",
    "                    self.post_param[key][1] = 1/np.sqrt(self.number_item_played[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CUCB\n",
    "\n",
    "Define the CUCB algorithm specific for the bipartite graph. CUCB computes optimistic indexes for each arm and find the decision that maximises linearly those indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class zCUCB():\n",
    "    \"\"\"\n",
    "    Only for the bipartite matching (for the moment)\n",
    "\n",
    "    Only for the Z graph bipartite matching\n",
    "    \n",
    "    Create the bp graph used in the experiment. Can be improved to receive graph parameter if needed.\n",
    "    \n",
    "    \n",
    "    input :\n",
    "        - m number of edges in one part of the graph\n",
    "        - n_init number of forced exploration set to 0 (not implemented on this environmemt)\n",
    "        - eps = 10^-8 small parameter trick for the stability\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, m,n_init = 0, eps = 10^-8):\n",
    "        \n",
    "        self.iteration = 0\n",
    "        self.m = m\n",
    "        self.d = m**2\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.initialization = False\n",
    "        \n",
    "        self.random_variable = random_variable\n",
    "        self.mu = dict()\n",
    "        self.playhist = []\n",
    "        self.number_item_played = dict()\n",
    "        \n",
    "        self.n_init = n_init\n",
    "        if self.n_init > 0:   \n",
    "            self.initialization = True\n",
    "        \n",
    "        self.iteration = 0\n",
    "        # Construct the dictionnary of parameter (could store all the parameter in a matrix n*n*2) \n",
    "        # but here for being more modulable it will be on \n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if j == 1+i or j == i: \n",
    "                    self.mu[i,j] = np.array([self.eps]) \n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                elif j==0 and i==self.m-1:\n",
    "                    self.mu[i,j] = np.array([self.eps]) \n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                else:     \n",
    "                    self.mu[i,j] = [False]\n",
    "                    self.number_item_played[i,j] = [False]\n",
    "                    \n",
    "        \n",
    "         \n",
    "    def reset(self):\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if j == 1+i or j == i: \n",
    "                    self.mu[i,j] = np.array([self.eps]) \n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                elif j==0 and i==self.m-1:\n",
    "                    self.mu[i,j] = np.array([self.eps]) \n",
    "                    self.number_item_played[i,j] = 0.\n",
    "                else:     \n",
    "                    self.mu[i,j] = [False]\n",
    "                    self.number_item_played[i,j] = [False]\n",
    "                    \n",
    "                self.number_item_played[i,j] = 0.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        This will choose the arm according to the bonus index of CUCB\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if self.iteration < self.n_init:\n",
    "            self.iteration +=1\n",
    "            arm_played = np.concatenate([np.arange(self.m)[:,None],np.arange(self.m)[:,None]],axis = 1)\n",
    "            return(arm_played)\n",
    "        \n",
    "        if self.iteration < 2*self.n_init:\n",
    "            self.iteration +=1\n",
    "            row = [self.m-1]+[i for i in range(self.m-1)]\n",
    "            arm_played = np.concatenate([np.array(row)[:,None],np.arange(self.m)[:,None]],axis = 1)\n",
    "            return(arm_played)\n",
    "        else:       \n",
    "            self.initialization = False\n",
    "            \n",
    "            \n",
    "        self.iteration +=1   \n",
    "        self.bonus_index = np.zeros((self.m,self.m))\n",
    "        \n",
    "        \n",
    "        for i in range(self.m):\n",
    "            for j in range(self.m):\n",
    "                if j == 1+i or j == i: \n",
    "                    self.bonus_index[i,j] = self.mu[i,j][0] + np.sqrt(1.5*np.log(self.iteration)/self.number_item_played[i,j])\n",
    "                elif j==0 and i==self.m-1:\n",
    "                    self.bonus_index[i,j] = self.mu[i,j][0] + np.sqrt(1.5*np.log(self.iteration)/self.number_item_played[i,j])\n",
    "        \n",
    "                \n",
    "        \n",
    "        row_ind, col_ind = linear_sum_assignment(-self.bonus_index) \n",
    "        # we have to maximize the reward (the scypy implementation minimizes the cast)\n",
    "\n",
    "        arm_played = np.concatenate([row_ind[:,None], col_ind[:,None]],axis=1)\n",
    "        #Trick to have the list of the arm played\n",
    "        \n",
    "        self.playhist.append(arm_played)\n",
    "\n",
    "        return arm_played\n",
    "\n",
    "\n",
    "    def update(self, observation):\n",
    "        \"\"\"\n",
    "        update the parameter of the arms\n",
    "        observation are a dictionary of reward\n",
    "        \"\"\"\n",
    "        \n",
    "        for key, valu in observation.items():\n",
    "            self.number_item_played[key] += 1\n",
    "            if  self.mu[key][0]:\n",
    "                    self.mu[key] = (self.mu[key]*(self.number_item_played[key]-1)+valu)/self.number_item_played[key]\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TS First Optimal play \n",
    "\n",
    "Performs the experiment to compute the first time the optimal arm is played"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Fixed delta\n",
    "\n",
    "For a fixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing the first time optimal is played\n",
    "\n",
    "# Params\n",
    "jump = 1\n",
    "ms = list(range(2,6,jump))\n",
    "n_trial = 1000 \n",
    "delta = 0.05\n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"delta\"] = delta\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "\n",
    "upper_limit = 20000000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_opti_{}_{}_{}_delta{:.3f}\".format(ms[0],ms[-1],jump,delta)\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n",
    "\n",
    "distrib_first_optimal = [[] for m in ms]\n",
    "\n",
    "\n",
    "for i,m in enumerate(ms[::-1]):\n",
    "    print(\"m = \",m) \n",
    "    optimal_arm = np.concatenate([np.arange(m)[:,None],np.arange(m)[:,None]],axis = 1)\n",
    "    \n",
    "    \n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "        \n",
    "        optimal_arm_played = False\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCThompson_sampling(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "        while player.initialization:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "        \n",
    "        while not optimal_arm_played and counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "\n",
    "                if counter%2000 == 0:\n",
    "                    print(\"Have still not played the optimal arm at time {}\".format(counter))\n",
    "                    \n",
    "                if (decision == optimal_arm).all():\n",
    "                    optimal_arm_played = True\n",
    "\n",
    "        distrib_first_optimal[-(i+1)].append(counter)   \n",
    "\n",
    "np.save(experimentfolder +\"\\\\Distribution\",distrib_first_optimal)\n",
    "\n",
    "#plot Result ECDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# experimentfolder = \"Experience_opti_4_6_1_Delta0.125\"\n",
    "# distrib_first_optimal = np.load(experimentfolder+\"\\\\Distribution.npy\")\n",
    "# ms = range(4,7)\n",
    "# n_init = 0\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,12))\n",
    "    plt.clf()\n",
    "    cdf = ECDF(distrib_first_optimal[i])\n",
    "    plt.plot([0]+list(np.sort(distrib_first_optimal[i])),[0]+list(cdf(np.sort(distrib_first_optimal[i]))),\"+-\",linewidth=5,markersize=12)\n",
    "#plt.title(\"Distribution of first optimal play  for b = {:.1f} and d = {:d} \".format(b,d),fontsize=20)   \n",
    "    plt.xlabel(r'$T_{opti}$', fontsize=50)\n",
    "    plt.tick_params(axis='x', labelsize=50)\n",
    "    plt.tick_params(axis='y', labelsize=50)\n",
    "    \n",
    "    plt.savefig(experimentfolder+'\\\\ECDF_m{}_l{}.pdf'.format(m,n_init))\n",
    "    plt.savefig(experimentfolder+'\\\\ECDF_m{}_l{}'.format(m,n_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3)\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='b', alpha=.3)\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "plt.ylabel(r'$T_{opti}$', fontsize=45)\n",
    "plt.xlabel('m', fontsize=45)\n",
    "plt.tick_params(axis='x', labelsize=45)\n",
    "plt.tick_params(axis='y', labelsize=45) \n",
    "\n",
    "plt.savefig(experimentfolder+'\\\\TS_opti_m{}_{}.pdf'.format(ms[0],ms[-1]))\n",
    "plt.savefig(experimentfolder+'\\\\TS_opti_m{}_{}'.format(ms[0],ms[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Merge results\n",
    "\n",
    "Provide a nice visualisation of the results, need to manually input  the folders used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_4_11_1_delta0.150\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(4,12,1)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3, color='b')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='b', alpha=.3)\n",
    "plt.plot(ms, Average_time, color='b', label = r\"$\\delta = 0.150$\")\n",
    "\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_4_6_1_Delta0.125\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(4,7,1)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3, color='g')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='g', alpha=.3)\n",
    "plt.plot(ms, Average_time, color='g', label = r\"$\\delta = 0.125$\")\n",
    "\n",
    "# with open(\"Experience_opti_4_6_1_Delta0.125\\\\params.pkl\", 'rb') as pickle_file:\n",
    "#     content = pickle.load(pickle_file)\n",
    "#     print(content[\"delta\"])\n",
    "\n",
    "\n",
    "\n",
    "distrib_first_optimal = np.load(\"Experience_opti_2_5_1_delta0.050\\\\Distribution.npy\")\n",
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "ms = range(2,6,1)\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3,color='r')\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='r', alpha=.3)\n",
    "plt.plot(ms, Average_time,color='r', label = r\"$\\delta = 0.05$\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "plt.ylabel(r'$T_{opti}$', fontsize=40)\n",
    "plt.xlabel('m', fontsize=40)\n",
    "plt.tick_params(axis='x', labelsize=40)\n",
    "plt.tick_params(axis='y', labelsize=40)\n",
    "plt.legend(fontsize = 40)\n",
    "\n",
    "plt.savefig(\"First_playing_time_Average_different_delta_zgraph\") \n",
    "plt.savefig(\"First_playing_time_Average_different_delta_zgraph.pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Unfixed delta\n",
    "\n",
    "For delta changing with the number of arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing the first time optimal is played\n",
    "\n",
    "# Params\n",
    "jump = 1\n",
    "ms = list(range(4,7,jump))\n",
    "n_trial = 1000 \n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "\n",
    "upper_limit = 20000000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_opti_{}_{}_{}_Delta0.5\".format(ms[0],ms[-1],jump)\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n",
    "\n",
    "distrib_first_optimal = [[] for m in ms]\n",
    "\n",
    "\n",
    "for i,m in enumerate(ms[::-1]):\n",
    "    print(\"m = \",m)\n",
    "    optimal_arm = np.concatenate([np.arange(m)[:,None],np.arange(m)[:,None]],axis = 1)\n",
    "    delta = 1/(2*m)\n",
    "    for iteration in range(n_trial):\n",
    "        if iteration%100 == 0:\n",
    "            print(\"begin_trial {}\".format(iteration))\n",
    "\n",
    "        optimal_arm_played = False\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCThompson_sampling(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "        while player.initialization:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "        \n",
    "        while not optimal_arm_played and counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                counter += 1\n",
    "\n",
    "                if counter%2000 == 0:\n",
    "                    print(\"Have still not played the optimal arm at time {}\".format(counter))\n",
    "                    \n",
    "                if (decision == optimal_arm).all():\n",
    "                    optimal_arm_played = True\n",
    "                    \n",
    "        distrib_first_optimal[-(i+1)].append(counter)\n",
    "\n",
    "np.save(experimentfolder +\"\\\\Distribution\",distrib_first_optimal)\n",
    "\n",
    "#plot Result ECDF\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    cdf = ECDF(distrib_first_optimal[i])\n",
    "    plt.plot([0]+list(np.sort(distrib_first_optimal[i])),[0]+list(cdf(np.sort(distrib_first_optimal[i]))),\"+-\",linewidth=3,markersize=10)\n",
    "#plt.title(\"Distribution of first optimal play  for b = {:.1f} and d = {:d} \".format(b,d),fontsize=20)   \n",
    "    plt.xlabel(r'$T_{opti}$', fontsize=20)\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    \n",
    "    plt.savefig(experimentfolder+'\\\\ECDF_m{}_l{}.pdf'.format(m,n_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Average_time = np.mean(distrib_first_optimal, axis =1)\n",
    "std = np.std(distrib_first_optimal, axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "plt.figure('Average Time',figsize = (16,9))\n",
    "plt.clf()\n",
    "plt.errorbar(ms, Average_time, std, linewidth=3)\n",
    "plt.fill_between(ms,Average_time - std, Average_time + std , color='b', alpha=.3)\n",
    "\n",
    "\n",
    "plt.xlim(xmin=0)\n",
    "\n",
    "#plt.title(\"Average time the optimal decision is played for the first time in function of d\",fontsize=20)\n",
    "plt.ylabel(r'$T_{opti}$', fontsize=20)\n",
    "plt.xlabel('m', fontsize=20)\n",
    "plt.tick_params(axis='x', labelsize=18)\n",
    "plt.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "plt.savefig(experimentfolder +\"\\\\Averagel{}.pdf\".format(n_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regret\n",
    "\n",
    "Compare the regret of CUCB with ESCB on the matching graph environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Fixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Params\n",
    "jump = 4\n",
    "ms = list(range(2,19,jump))\n",
    "n_trial = 100\n",
    "delta = 0.05\n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"delta\"] = delta\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "Param_dict[\"Vard\"] = 1\n",
    "\n",
    "upper_limit = 100000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_Regret_{}_{}_{}_delta{:.3f}_vardelta{}\".format(ms[0],ms[-1],jump,delta,Param_dict[\"Vard\"])\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "regrets = np.zeros((len(ms),n_trial,upper_limit))\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    print(\"m = \",m)\n",
    "    if Param_dict[\"Vard\"]:\n",
    "        delta = Param_dict[\"Vard\"]/m\n",
    "    \n",
    "    for iteration in range(n_trial):\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCThompson_sampling(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "    \n",
    "        \n",
    "        while counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                \n",
    "                regrets[i,iteration,counter] = regret\n",
    "                \n",
    "                counter += 1\n",
    "                if counter%5000 == 0:\n",
    "                    print(\"play number {}\".format(counter))\n",
    "\n",
    "np.save(experimentfolder +\"\\\\TSregrets\",regrets)                    \n",
    "cumulative_regretsTS = np.cumsum(regrets, axis = 2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,m in enumerate(ms):\n",
    "    if Param_dict[\"Vard\"]:\n",
    "        delta = Param_dict[\"Vard\"]/m\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    #plt.title(\" m = {} , b = {:.4f}\".format(m,b), fontsize=20)\n",
    "    for k in np.random.randint(0,n_trial,20):\n",
    "        plt.plot(cumulative_regretsTS[i,k,::])\n",
    "    plt.plot(np.mean(cumulative_regretsTS[i,::,::],axis = 0), label = \"Mean Regret\",linewidth=5)\n",
    "    maxi95 = np.argsort(cumulative_regretsTS[i,::,-1])[int(n_trial*0.975)]\n",
    "    maxi95 = cumulative_regretsTS[i,maxi95,::]\n",
    "    \n",
    "    mini95 = np.argsort(cumulative_regretsTS[i,::,-1])[int(n_trial*0.025)]\n",
    "    mini95 = cumulative_regretsTS[i,mini95,::]\n",
    "    \n",
    "    plt.fill_between(range(upper_limit),mini95,maxi95, color='b', alpha=.1)\n",
    "    \n",
    "    plt.xlabel('t', fontsize=18)\n",
    "    plt.ylabel('Regret', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(experimentfolder+'\\\\TS_regret_m{}_l{}.pdf'.format(m,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### CUCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "regrets = np.zeros((len(ms),n_trial,upper_limit))\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    print(\"m = \",m)\n",
    "    if Param_dict[\"Vard\"]:\n",
    "        delta = Param_dict[\"Vard\"]/m\n",
    "    \n",
    "    for iteration in range(int(n_trial/10)):\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCUCB(m,n_init = 1)\n",
    "\n",
    "        counter = 0\n",
    "    \n",
    "        \n",
    "        while counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                \n",
    "                regrets[i,iteration,counter] = regret\n",
    "                \n",
    "                counter += 1\n",
    "                if counter%5000 == 0:\n",
    "                    print(\"play number {}\".format(counter))\n",
    "\n",
    "np.save(experimentfolder +\"\\\\CUCBregrets\",regrets)         \n",
    "cumulative_regretscucb = np.cumsum(regrets, axis = 2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,m in enumerate(ms):\n",
    "    if Param_dict[\"Vard\"]:\n",
    "        delta = Param_dict[\"Vard\"]/m\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    #plt.title(\" m = {} , b = {:.4f}\".format(m,b), fontsize=20)\n",
    "    for k in np.random.randint(0,n_trial,20):\n",
    "        plt.plot(cumulative_regretscucb[i,k,::])\n",
    "    plt.plot(np.mean(cumulative_regretscucb[i,::,::],axis = 0), label = \"Mean Regret\",linewidth=5)\n",
    "    maxi95 = np.argsort(cumulative_regretscucb[i,::,-1])[int(n_trial*0.975)]\n",
    "    maxi95 = cumulative_regretscucb[i,maxi95,::]\n",
    "    \n",
    "    mini95 = np.argsort(cumulative_regretscucb[i,::,-1])[int(n_trial*0.025)]\n",
    "    mini95 = cumulative_regretscucb[i,mini95,::]\n",
    "    \n",
    "    plt.fill_between(range(upper_limit),mini95,maxi95, color='b', alpha=.1)\n",
    "    \n",
    "    plt.xlabel('t', fontsize=18)\n",
    "    plt.ylabel('Regret', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(experimentfolder+'\\\\CUCB_regret_m{}.pdf'.format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Merge results\n",
    "Provide a nice visualisation of the results, need to manually input the folders used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "experimentfolder = \"Experience_Regret_2_18_4_delta0.050_vardelta1\"\n",
    "ms = range(2,18,4)\n",
    "\n",
    "regrets = np.load(experimentfolder+\"\\\\CUCBregrets\"+\".npy\")[:-1:,::,::]\n",
    "cumulative_regretscucb = np.cumsum(regrets, axis = 2)\n",
    "regrets = np.load(experimentfolder+\"\\\\TSregrets\"+\".npy\")[:-1:,::,::]\n",
    "cumulative_regretsTS = np.cumsum(regrets, axis = 2)\n",
    "\n",
    "Regretcucb = np.mean(cumulative_regretscucb[::,::,-1],axis = 1)\n",
    "stdcucb = np.std(cumulative_regretscucb[::,::,-1], axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "RegretTS = np.mean(cumulative_regretsTS[::,::,-1],axis = 1)\n",
    "stdTS = np.std(cumulative_regretsTS[::,::,-1], axis =1)/np.sqrt(n_trial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(\"Final regret\",figsize = (16,9))\n",
    "plt.clf()\n",
    "\n",
    "plt.errorbar(ms, Regretcucb, stdcucb, linewidth=1)\n",
    "plt.plot(ms,Regretcucb, label = \"CUCB\",linewidth=5,color='b')\n",
    "plt.fill_between(ms,Regretcucb - stdcucb, Regretcucb + stdcucb , color='b', alpha=.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.errorbar(ms, RegretTS, stdTS, linewidth=1,color='r')\n",
    "plt.plot(ms,RegretTS, label = \"TS\",linewidth=5,color='r')\n",
    "plt.fill_between(ms,RegretTS - stdTS, RegretTS + stdTS , color='r', alpha=.2)\n",
    "\n",
    "\n",
    "plt.xlabel('m', fontsize=40)\n",
    "plt.ylabel('Regret', fontsize=40)\n",
    "plt.tick_params(axis='x', labelsize=40)\n",
    "plt.tick_params(axis='y', labelsize=40)\n",
    "plt.legend(fontsize=40)\n",
    "plt.savefig(experimentfolder+'\\\\final_regret_comparison_m{}_{}.pdf'.format(ms[0],ms[-1]))\n",
    "plt.savefig(experimentfolder+'\\\\final_regret_comparison_m{}_{}'.format(ms[0],ms[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Unfixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "jump = 1\n",
    "ms = list(range(4,12,jump))\n",
    "n_trial = 50 \n",
    "n_init = 0\n",
    "random_variable = \"bernoulli\"\n",
    "post_distrib = \"beta\"\n",
    "\n",
    "Param_dict = {}\n",
    "\n",
    "Param_dict[\"ms\"] = ms\n",
    "Param_dict[\"n_trial\"] = n_trial\n",
    "Param_dict[\"delta\"] = delta\n",
    "Param_dict[\"random_variable\"] = random_variable\n",
    "Param_dict[\"post_distrib\"] = post_distrib\n",
    "\n",
    "upper_limit = 10000\n",
    "\n",
    "# Save Folder\n",
    "experimentfolder = \"Experience_Regret_{}_{}_{}_Delta0.5\".format(ms[0],ms[-1],jump)\n",
    "\n",
    "if not os.path.exists(experimentfolder):\n",
    "    os.makedirs(experimentfolder)\n",
    "\n",
    "f = open(experimentfolder + \"\\\\params.pkl\",\"wb\")\n",
    "pickle.dump(Param_dict,f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "regrets = np.zeros((len(ms),n_trial,upper_limit))\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    print(\"m = \",m)\n",
    "    delta = 1/(2*m)\n",
    "    \n",
    "    for iteration in range(n_trial):\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCThompson_sampling(m,n_init = n_init)\n",
    "\n",
    "        counter = 0\n",
    "    \n",
    "        \n",
    "        while counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                \n",
    "                regrets[i,iteration,counter] = regret\n",
    "                \n",
    "                counter += 1\n",
    "                if counter%5000 == 0:\n",
    "                    print(\"play number {}\".format(counter))\n",
    "                    \n",
    "cumulative_regrets = np.cumsum(regrets, axis = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    #plt.title(\" m = {} , b = {:.4f}\".format(m,b), fontsize=20)\n",
    "    for k in np.random.randint(0,n_trial,20):\n",
    "        plt.plot(cumulative_regrets[i,k,::])\n",
    "    plt.plot(np.mean(cumulative_regrets[i,::,::],axis = 0), label = \"Mean Regret\",linewidth=5)\n",
    "    maxi95 = np.argsort(cumulative_regrets[i,::,-1])[int(n_trial*0.975)]\n",
    "    maxi95 = cumulative_regrets[i,maxi95,::]\n",
    "    \n",
    "    mini95 = np.argsort(cumulative_regrets[i,::,-1])[int(n_trial*0.025)]\n",
    "    mini95 = cumulative_regrets[i,mini95,::]\n",
    "    \n",
    "    plt.fill_between(range(upper_limit),mini95,maxi95, color='b', alpha=.1)\n",
    "    \n",
    "    plt.xlabel('t', fontsize=18)\n",
    "    plt.ylabel('Regret', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(experimentfolder+'\\\\TS_regret_m{}_l{}.pdf'.format(m,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### CUCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "regrets = np.zeros((len(ms),n_trial,upper_limit))\n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    print(\"m = \",m)\n",
    "    delta = 1/(2*m)\n",
    "    for iteration in range(n_trial):\n",
    "\n",
    "        env = zgraph(m, delta = delta)\n",
    "        player = zCUCB(m,n_init = 1)\n",
    "\n",
    "        counter = 0\n",
    "    \n",
    "        \n",
    "        while counter < upper_limit:\n",
    "                decision = player.play()\n",
    "                reward_dict, regret = env.draw(decision)\n",
    "                player.update(reward_dict)\n",
    "                \n",
    "                regrets[i,iteration,counter] = regret\n",
    "                \n",
    "                counter += 1\n",
    "                if counter%5000 == 0:\n",
    "                    print(\"play number {}\".format(counter))\n",
    "                    \n",
    "cumulative_regrets = np.cumsum(regrets, axis = 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i,m in enumerate(ms):\n",
    "    plt.figure(i,figsize = (16,9))\n",
    "    plt.clf()\n",
    "    #plt.title(\" m = {} , b = {:.4f}\".format(m,b), fontsize=20)\n",
    "    for k in np.random.randint(0,n_trial,20):\n",
    "        plt.plot(cumulative_regrets[i,k,::])\n",
    "    plt.plot(np.mean(cumulative_regrets[i,::,::],axis = 0), label = \"Mean Regret\",linewidth=5)\n",
    "    maxi95 = np.argsort(cumulative_regrets[i,::,-1])[int(n_trial*0.975)]\n",
    "    maxi95 = cumulative_regrets[i,maxi95,::]\n",
    "    \n",
    "    mini95 = np.argsort(cumulative_regrets[i,::,-1])[int(n_trial*0.025)]\n",
    "    mini95 = cumulative_regrets[i,mini95,::]\n",
    "    \n",
    "    plt.fill_between(range(upper_limit),mini95,maxi95, color='b', alpha=.1)\n",
    "    \n",
    "    plt.xlabel('t', fontsize=18)\n",
    "    plt.ylabel('Regret', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.savefig(experimentfolder+'\\\\CUCB_regret_m{}.pdf'.format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
